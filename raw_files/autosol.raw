
<!--REMARK PHENIX TITLE START  Put your title here>


<H4><U>Automated structure solution with AutoSol</U></H4>


<!--REMARK PHENIX TITLE END-->

<!--REMARK PHENIX BODY START   Put your text here. 
Anything enclosed in header html H4 H5 etc will go in the table of contents>


      <P><H5><U>Author(s)</U></H5><P>
<UL><LI>AutoSol Wizard: Tom Terwilliger
<LI>PHENIX GUI and PDS Server: Nigel W. Moriarty
<LI>HYSS: Ralf W. Grosse-Kunstleve and Paul D. Adams
<LI>Phaser: Randy J. Read, Airlie J. McCoy and Laurent C. Storoni
<LI>SOLVE:  Tom Terwilliger
<LI>RESOLVE: Tom Terwilliger
<LI>TEXTAL: K. Gopal, T.R. Ioerger, R.K. Pai, T.D. Romo, J.C. Sacchettini
<LI>phenix.refine: Ralf W. Grosse-Kunstleve, Peter Zwart and Paul D. Adams
<LI>phenix.xtriage: Peter Zwart
</UL>

      <P><H5><U>Purpose</U></H5><P>
The AutoSol Wizard uses HYSS, SOLVE, Phaser, RESOLVE, TEXTAL, xtriage and 
phenix.refine to solve a structure and generate experimental phases
with the MAD, MIR, SIR, or SAD methods. 
The Wizard begins with
datafiles (.sca, .hkl, etc) containing amplitidues of structure factors,
identifies heavy-atom sites, calculates phases, carries out density
modification and NCS identification, and builds and refines a 
preliminary model.

      <P><H5><U>Usage</U></H5><P>
<P>The AutoSol Wizard can be run from the PHENIX GUI, from the command-line, 
and from keyworded script files.  All three versions are identical except
in the way that they take commands from the user. 
See 
<a href="running-wizards.htm">
Running a Wizard from a GUI, the command-line, or a script</a> 
for details of how to run a Wizard.
The command-line version will be described here, except for MIR and multiple
datasets, which can only be run with the GUI or with a script.
<P>
<P><H5>How the AutoSol Wizard works</H5><P>
<P>The basic steps that the AutoSol Wizard carries out are
described below. They are: Setting up inputs,
Analyzing and scaling the data, Finding heavy-atom 
(anomalously-scattering atom) sites, Scoring of heavy-atom solutions,
Phasing, Density modification (including NCS averaging), and
Preliminary model-building and refinement. The data for structure solution
are grouped into Datasets and solutions are stored in Solution objects.

<P><H5><I>Setting up inputs</I></H5><P>
<P>The AutoSol Wizard expects the following basic information:
<P>(1) a datafile name (w1.sca or data=w1.sca)
<P>(2) a sequence file (seq.dat or seq_file=seq.dat)
<P>(3) how many sites to look for (2 or sites=2)
<P>(4) what the anomalously-scattering atom is (Se or atom_type=Se)
<P>(5) If you have SAD or MAD data, then it is helpful to add 
f_prime and f_double_prime for each wavelength.
<P> You can also specify many other parameters, including resolution,
number of sites, whether to search in a thorough or quick fashion, how
thoroughly to build a model, etc. If you have a heavy-atom solution 
from a previous run or another approach, you can read it in directly as well.

<P><H5><I>Datasets and Solutions in AutoSol </I></H5><P>
<P>AutoSol breaks down the data for a structure solution into
datasets, where a dataset is a set of data that corresponds to a single
set of heavy-atom sites.  An entire MAD dataset is a single dataset.  An
MIR structure solution consists of several datasets (one for each 
native-derivative combination).  A MAD + SIR structure has one dataset
for the MAD data and a second dataset for the SIR data.  The heavy-atom
sites for each dataset are found separately (but using difference Fouriers
from any previously-solved datasets to help). In the phasing step all the
information from all datasets is merged into a single set of phases.
<P>The AutoSol wizard uses a "Solution" object to keep track of
heavy-atom solutions and the phased datasets that go with them. There are two
types of Solutions: those which consist of a single dataset (Primary
Solutions) and those that are combinations of datasets (Composite Solutions).
"Primary" Solutions have information on the datafiles that were part of the
dataset and on the heavy-atom sites for this dataset. Composite Solutions are
simply sets of Primary Solutions, with associated origin shifts.
The hand of the heavy-atom or anomalously-scattering atom substructure
is part of a Solution, so if you have two datatsets, each with two Solutions
related by inversion, then AutoSol would normally construct four different
Composite Solutions from these and score each one as described below.

<P><H5><I>Analyzing and scaling the data</I></H5><P>
<P>The AutoSol Wizard analyzes input datasets with phenix.xtriage to 
identify twinning and other conditions that may require special care. The 
data is scaled with SOLVE. For MAD data, FA values are calculated as well.

<P>Note on anisotropy corrections:

<P>The AutoSol wizard will apply an anistropy correction to all the raw
experimental data if any of the files in the first dataset read in have a very
strong anisotropy. You can tell the Wizard how much anisotropy there must be
before applying this correction by default using the keywords

<PRE style="face=courier">correct_aniso=True  # (if True or False then always or never apply correction)

delta_b_for_auto_correct_aniso=20  # correct if range of anisotropic B 
                                   #is greater than 20

ratio_b_for_auto_correct_aniso=1.5  #correct if the ratio of the largest 
                                  #to smallest anisotropic B is greater than 1.5
</PRE>
<P>If an anisotropy correction is applied then a separate 
refinement file must be specified if refinement is to be carried out. 
This is because it is best to
refine against data that have not been corrected for anisotropy (instead
applying the correction as part of refinement).

<P><H5><I>Finding heavy-atom (anomalously-scattering atom) sites</I></H5><P>
<P>The AutoSol Wizard uses HYSS to find heavy-atom sites. 
The result of this step is a list of possible heavy-atom
solutions for a dataset. For SIR or SAD data,
the isomorphous or anomalous differences, respectively are used as input to
HYSS. For MAD data, the anomalous differences at each wavelength, and 
the FA estimates of complete heavy-atom structure factors from SOLVE are each
used as separate inputs to HYSS. Each heavy-atom substructure obtained from
HYSS corresponds to a potential solution. In space groups where the heavy-atom
structure can be either hand, a pair of enantiomorphic solutions is saved for
each run of HYSS. 

<P><H5><I>Running AutoSol separately in related space groups </I></H5><P>
AutoSol will check for the opposite hand of the heavy-atom solution, and
at the same time it will check for the opposite hand of your space group
(It will invert the heavy-atom solution from HYSS and invert the hand of the
space group at the same time). Therefore you do not need to run AutoSol
twice for space groups that are chiral (for example P41).  The
corresponding inverse space groups will be checked automatically (P43 ).
<p>If there are possibilities for your space group other than the inverse
hand of the space group, then
you should test them all, one at a time. 
For example if you were not able to measure
00l reflections in a hexagonal space group, your
space group  might be P6, P61, P62, P63, P64 or P65. 
In this case you  would have to run it in P6, P61 P62 and P63 (and then
P65 and P64 will be done automatically as the inverses of P61 and P62).
Normally only one of these will give a plausible solution.

<P><H5><I>Scoring of heavy-atom solutions</I></H5><P>
Potential heavy-atom solutions are scored based on a set of criteria 
(CC, RFACTOR, SKEW, FOM, NCS_OVERLAP, TRUNCATION, REGIONS, SD; 
described below), using 
either a Bayesian estimate, a linear regression, or a
Z-score system to put all the scores on a common scale and to combine them
into a single overall score. 
<p>The overall
scoring method chosen (BAYES-CC or Z-SCORE) is determined by the
value of the keyword overall_score_method. The default is BAYES-CC.
<p>Note that for all scoring methods, the map that is being evaluated,
and the estimates of map-perfect-model correlation, refer to 
the experimental electron density map, not the density-modified map. 


<P><b>Bayesian CC scores (BAYES-CC).</b> Bayesian estimates of the quality
of experimental electron density maps are obtained using
data from a set of previously-solved datasets.
The standard scoring criteria were evaluated for 1905 potential solutions in a
set of 246 MAD, SAD, and MIR datasets.  As each dataset had previously been
solved, the correlation between the refined model and each experimental
map (CC_PERFECT) could be calculated for each solution (after offsetting
the maps to account for origin differences).
Histograms were tabulated of the number of instances
that a scoring criterion (e.g., SKEW) had various possible values, as a function
of the CC_PERFECT of the corresponding experimental map to the refined model.
These histograms yield the relative probability of measuring a particular value
of that scoring criterion (SKEW), given the value of CC_PERFECT. Using
Bayes' rule, these probabilities can be used to estimate the relative 
probabilities of values of CC_PERFECT given the value of each
scoring criterion for a particular electron density map. 
The mean estimate (BAYES-CC) is reported (multiplied x 100),
 with a +/-2SD estimate of the uncertainty in this estimate of CC_PERFECT.
<p>The BAYES-CC values are
estimated independently for each scoring criterion used, and also from all
those selected with the keyword score_type_list and not selected
with the keyword skip_score_list. 

<P><b>Z-scores (Z-SCORE).</b> 
The Z-score for one criterion for a particular solution is given by,
<PRE style="face=courier">Z= (Score - mean_random_solution_score)/(SD_of_random_solution_scores)
</PRE>
where Score is the score for this solution, mean_random_solution_score is
the mean score for a solution with randomized  phases, and 
SD_of_random_solution_scores is the standard deviation of the scores of
solutions with randomized phases.
<P>To create a total score based on Z-scores, the Z-scores for each criterion
are simply summed.

<P> The principal scoring criteria are:
<P>(1) Correlation of map-phased electron density map with experimentally-
phased map (CC). The statistical density modification in RESOLVE allows the 
calculation of map-based phases that are (mostly) independent of the 
experimental phases. The phase information in statistical density 
modification comes from two sources: your experimental phases and 
maximization of the agreement of the map with expectations (such as
a flat solvent region). Normally the 
phase probabilities from these two sources are merged together, yielding 
your density-modified phases. This score is calculated based on the 
correlation of the phase information from these two sources before
combining them, and is a good indication of the quality of the
experimental phases. 
This criterion is used in scoring by default.
<P>(2) The R-factor for density modification (R-Factor). Statistical 
density modification provides an estimate of structure factors that is
(mostly) independent of the measured structure factors, so the R-factor
between FC and Fobs is a good measure of the quality of experimental phases.
This criterion is used in scoring by default.
<P>(3) The skew (third moment or normalized &lt;rho**3&gt;) of the density in an
electron density map is a good measure of its quality, because a random 
map has a skew of zero (density histograms look like a Gaussian), while a
good map has a very positive skew (density histograms very strong near zero,
but many points with very high density). 
This criterion is used in scoring by default.
<P>(4) Non-crystallographic symmetry (NCS overlap).  The presence of 
NCS in a map is a nearly-positive indication that the map is good, or has
some correct features. The AutoSol Wizard uses symmetry in heavy-atom
sites to suggest NCS, and RESOLVE identifies the actual correlation of
NCS-related density for the NCS overlap score. 
This score is used by default if NCS is present in the Z-score method of 
scoring. 
<P>(5) Figure of merit (FOM). The figure of merit of phasing is a good 
indicator of the internal consistency of a solution. This score is not
normalized by the SD of randomized phase sets (as that has no meaning; rather
a standard SD=0.05 is used).
This score is used by default if NCS is present in the Z-score method of 
scoring and in the Bayesian CC estimate method.
<P>(6) Map correlation after truncation (TRUNCATION). 
Dummy atoms (the same number as
estimated non-hydrogen atoms in the structure) are placed in positions
of high density of the map, and a new map is calculated based on these
atomic positions.  The correlation of these maps is calculated
after adjusting an overall B-value for the dummy atoms to maximize the
correlation.   A good map will show a high correlation of these maps.
This score is by default not used.
<P>(7) Number of contiguous regions per 100 A**3 comprising top 5% of density
in map (REGIONS).  
The top 5% of points in the map are marked, and the number of 
contiguous regions that result are counted, and divided by the volume
 of the asymmetric unit, then multiplied by 100. A good map will have just 
a few contiguous regions at a high contour level, a poor map will have many
isolated peaks.
This score is by default not used.
<p>(8) Standard deviation of local rms density (SD). The local rms
density in the map is calculated using a smoothing radius of 3 times
the high-resolution cutoff (or 6 A, if less than 6A). 
Then the standard deviation
of the local rms, normalized to the mean value of the local rms, is
reported. This criteria will be high if there are regions of high local
rms (the macromolecule) and separate regions of low local rms (the solvent)
and low if the map is random. 
This score is by default not used.

<P><H5><I>Phasing</I></H5><P>
<P>The AutoSol Wizard uses Phaser to calculate experimental phases from
SAD data, and SOLVE to calculate phases from MIR, MAD, and multiple-dataset
cases.
<P><H5><I>Density modification (including NCS averaging)</I></H5><P>
<P>The AutoSol Wizard uses RESOLVE to carry out density modification. It 
identifies NCS from symmetries in heavy-atom sites with RESOLVE and applies
this NCS if it is present in the electron density map.
<P><H5><I>Preliminary model-building and refinement</I></H5><P>
<P>The AutoSol Wizard carries out one cycle of model-building and
refinement after obtaining density-modified phases. The model-building can be
with RESOLVE or with TEXTAL.  The refinement is carried out with phenix.refine.

<P><H5><I>Resolution limits in AutoSol</I></H5><P>
<P>There are several resolution limits used in AutoSol. You can leave them
all to default, or you can set any of them individually. Here is a list of
these limits and how their default values are set:
<TABLE width=670 border=1>
 <TBODY>
  <TR>
    <TD>Name</TD>
    <TD>Description</TD>
    <TD>How default value is set </TD>
  </TR>
  <TR>
    <TD>resolution</TD>
    <TD>Overall resolution for a dataset</TD>
    <TD>Highest resolution for any datafile in this dataset. For
       multiple datasets, the highest resolution for any dataset </TD>
  </TR>
  <TR>
    <TD>refinement_resolution</TD>
    <TD>Resolution for refinement</TD>
    <TD>value of &quot;resolution&quot;</TD>
  </TR>
  <TR>
    <TD>resolution_build</TD>
    <TD>Resolution for model-building</TD>
    <TD>value of &quot;resolution&quot;</TD>
  </TR>
  <TR>
    <TD>res_phase</TD>
    <TD>Resolution for phasing for a dataset</TD>
    <TD>If phase_full_resolution=True then 
            use value of &quot;resolution&quot;. 
         Otherwise, use value of &quot;recommended_resolution&quot;
         based on analysis of signal-to-noise in dataset.</TD>
  </TR>
  <TR>
    <TD>res_eval</TD>
    <TD>Resolution for evaluation of solution quality</TD>
    <TD>value of &quot;resolution&quot; or 2.5 A, whichever is
        lower resolution.  </TD>
  </TR>
 </TBODY>
</TABLE>

<P><H5><I>Output files from AutoSol</I></H5><P>
<P>When you run AutoSol the output files will be in a subdirectory with
your run number: 
<PRE style="face=courier">AutoSol_run_1_/
</PRE>
<P>The key output files that are produced are:

<P><UL><LI>A summary file listing the results of the run and 
the other files produced:
<PRE style="face=courier">AutoSol_summary.dat  # overall summary
</PRE>

<P><LI>A warnings file listing any warnings about the run
<PRE style="face=courier">AutoSol_warnings.dat  # any warnings
</PRE>

<P><LI>A file that lists all parameters and knowledge accumulated
by the Wizard during the run (some parts are binary and are not printed)
<PRE style="face=courier">AutoSol_Facts.dat   # all Facts about the run
</PRE>

<P><LI>NCS information  (if any)
<PRE style="face=courier">AutoSol_15.ncs_spec   # NCS information. The number is the solution number
</PRE>

<P><LI>Experimental phases and HL coefficients
<PRE style="face=courier">solve_15.mtz  # either solve or phaser depending on which was run
phaser_15.mtz
</PRE>

<P><LI>Density-modified phases from RESOLVE
<PRE style="face=courier">current_cycle_map_coeffs.mtz  # map coefficients (density modified phases)
resolve_15.mtz   # density-modified phases; same as above
</PRE>
For either of these, use FP PHIM FOMM for PHI F FOM.

<P><LI>An mtz file for use in refinement
<PRE style="face=courier">exptl_fobs_phases_freeR_flags_15.mtz  # F Sigma HL coeffs, freeR-flags for refinement
</PRE>

<P><LI>Heavy atom sites in PDB format
<PRE style="face=courier">ha_15.pdb_formatted.pdb
</PRE>

<P><LI>Current preliminary model and evaluation of model
<PRE style="face=courier">current_cycle.pdb
current_cycle_eval.log
</PRE>

</UL>

<P><H5>How to run the AutoSol Wizard</H5><P>
<P>  Running the AutoSol Wizard is easy.  From the command-line you can 
type:
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se f_prime=-8 f_double_prime=4.5
</PRE>
<P>The AutoSol Wizard will assume that w1.sca is a datafile (because it
ends in .sca and is a file) and that seq.dat is a sequence file, that there
are 2 heavy-atom sites, and that the heavy-atom is Se. The f_prime and
f_double_prime values are set explicitly
<P> You can also specify each of these things directly:
<PRE style="face=courier">phenix.autosol data=w1.sca seq_file=seq.dat sites=2 \
   atom_type=Se f_prime=-8 f_double_prime=4.5
</PRE>
<P>You can specify many more parameters as well. See  the list of
keywords, defaults and descriptions at the end of this page and
also general information about running Wizards at
<a href="running-wizards.htm">
Running a Wizard from a GUI, the command-line, or a script</a> 
for how to do this.  Some of the most common parameters are:
<PRE style="face=courier">sites=3     # 3 sites
sites_file=sites.pdb  # ha sites in PDB or fractional xyz format
atom_type=Se   # Se is the heavy-atom
seq_file=seq.dat   # sequence file (1-aa code, separate chains with >>>>)
quick=True  # try to find sites quickly
data=w1.sca  # input datafile
f_prime=-5  # f-prime value for SAD
f_double_prime=4.5  # f-double-prime value for SAD
</PRE>

<P><H5><I>Model viewing during model-building with the Coot-PHENIX interface 
</I></H5><P>
<P>The AutoSol Wizard allows you to view the current best model that is
produced by the automated model-building process.  This capability is 
identical to the view/edit model procedure available in the 
<A href="autobuild.htm"> AutoBuild </A>
Wizard. Normally you would use it just to view the model in AutoSol, and
to view and edit a model in 
<A href="autobuild.htm"> AutoBuild </A> .
<p>The PHENIX-Coot interface is accessible through the GUI and 
via the command-line.
Using the GUI, when a model has been produced by the AutoSol Wizard, 
you can double-click the  button on the GUI labelled 
<b>View/edit files with coot</b> to start Coot with your current map and model.
If you are running from the command-line, you can open a new window and
type:
<PRE style="face=courier">phenix.autobuild coot </PRE>
which will do the same (provided the necessary map and model are ready).
<p>When Coot has been loaded, your map and model will be displayed along with
a <b>PHENIX-Coot Interface</b> window. If you want, 
you can edit your model and then save 
it, giving it back to PHENIX with the button labelled something like
<b>Save model as COMM/overall_best_coot_7.pdb</b>. This button creates the
indicated file and also tells PHENIX to look for this file and to try and
include the contents of the model in the building process. In AutoSol, only
the main-chain atoms of the model you save are considered, and
the side-chains are ignored.  Ligands 
and solvent in the model are ignored as well.
<p>As the AutoSol Wizard continues to build new models and create new maps,
you can update in the PHENIX-Coot Interface to the current best model and
map with the button <b>Update with current files from PHENIX</b>.
      <P><H5><U>Examples</U></H5><P>
<P><H5>SAD dataset</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se f_prime=-8 f_double_prime=4.5
</PRE>
The sequence file is used to estimate the solvent content of the
crystal and for model-building.
Note that for a SAD dataset the value of f_prime and f_double_prime are not 
critical. If you are off by a factor of 2 on  f_double_prime, 
the refined occupancies of heavy-atom sites might be 1/2 their correct
values.

<P><H5>SAD dataset specifying solvent fraction</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se f_prime=-8 f_double_prime=4.5 \
    solvent_fraction=0.45
</PRE>
This will force the solvent fraction to be 0.45.  This illustrates a
general feature of the Wizards: they will try to estimate values of
parameters, but if you input them directly, they will use your input
values.

<P><H5>SAD dataset without model-building</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se f_prime=-8 f_double_prime=4.5 \
    build=False
</PRE>
This will carry out the usual structure solution, but will skip model-building

<P><H5>SAD dataset, building RNA instead of protein</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se f_prime=-8 f_double_prime=4.5 \
    chain_type=RNA
</PRE>
This will carry out the usual structure solution, but will build an RNA
chain.  For DNA, specify chain_type=DNA.  You can only build one type of
chain at a time in the AutoSol Wizard. To build protein and DNA, use 
the <A href="autobuild.htm"> AutoBuild </A>
Wizard and run it first with chain_type=PROTEIN, then
run it again specifying the protein 
model as input_lig_file_list=proteinmodel.pdb
and with chain_type=DNA.

<P><H5>SAD dataset, selecting a particular dataset from an MTZ file</H5><P>
If you have an input MTZ file with more than one anomalous dataset, you can 
type something like:
<PRE style="face=courier">phenix.autosol w1.mtz seq.dat 2 Se f_prime=-8 f_double_prime=4.5 \
labels='F SIGF DANO SIGDANO'
</PRE>
This will carry out the usual structure solution, but will choose the
input data columns based on the  labels: 'F SIGF DANO SIGDANO'. If you run the
AutoSol Wizard with SAD data and an MTZ file containing more than one
anomalous dataset and don't tell it which one to use, 
all possible values of labels are 
printed out for you so that you can just paste the one you want in.
<P>You can also find out all the possible label strings to use by typing:
<PRE style="face=courier">phenix.autosol display_labels=w1.mtz  # display all labels for w1.mtz
</PRE>

<P><H5>MRSAD -- SAD dataset with an MR model; Phaser SAD phasing including the model</H5>

<p>If you are carrying out SAD phasing with Phaser, you can carry out a combination of 
molecular replacement phasing and SAD phasing (MRSAD) by adding
a single new keyword to your AutoSol run:
</p><PRE style="face=courier">input_partpdb_file=MR.pdb
</PRE>
In this case the MR.pdb file will be used as a partial model in a maximum-likelihood
SAD phasing calculation with Phaser to calculate phases and identify
sites in Phaser, and the combined MR+SAD phases will be written out.
<p>NOTE: At the moment the AutoBuild Wizard is not equipped to use these combined
phases optimally in iterative model-building, density modification
and refinement, because they contain both experimental phase information and model
information. It is therefore possible that the resulting phases are biased by your 
MR model, and that this bias will not go away during iterative model-building because it
is continually fed back in.

<P><H5>Using an MR model to find sites and as a source of phase information (method #2 for MRSAD)</H5>

You can also combine MR information with SAD phases (see J. P. Schuermann and 
J. J. Tanner Acta Cryst. (2003). D59, 1731-1736 ) in PHENIX
by running the three wizards AutoMR, AutoSol, and AutoBuild one after the
other.  This method does not use the partial model and the
anomalous information in the SAD dataset simultaneously as the above Phaser 
maximum-likelihood method does. On the other hand, the phases obtained 
in this method are independent of the model, so that combining them 
afterwards does not introduce model bias. (It is not yet clear which is the better
approach, so you may wish to try both.) Additionally, this approach can be used with
any method for phasing.

<p>Here is a set of three simple commands to do this:
<p>First run <b>AutoMR</b> to find the molecular replacement solution, but 
don't rebuild it yet:
<PRE style="face=courier">phenix.automr gene-5.pdb infl.sca copies=1 \
  RMS=1.5 mass=9800 rebuild_after_mr=False
</PRE>
<p>Now your MR solution is in <b>AutoMR_run_1_/MR.1.pdb</b> and 
phases are in <b>AutoMR_run_1_/MR.1.mtz</b>.  Use these phases as input
to AutoSol, along with some weak SAD data, still not building any new models:
<PRE style="face=courier"> phenix.autosol data=infl.sca \
 input_phase_file=AutoMR_run_1_/MR.1.mtz input_phase_labels="F PHIC FOM"   \
seq_file=sequence.dat build=False
</PRE>
<p> note that we have specified the data columns for F PHI and FOM in the
input_phase_file. For input_phase_file you must specify all three of these (if 
you leave out FOM it will set it to zero). </p>
<p>AutoSol will write an MTZ file with experimental phases to <b>phaser_xx.mtz</b> where xx depends on how many solutions are considered during the run.  The
next command for running AutoBuild  you will need to edit depending 
on the value of xx:
<PRE style="face=courier"> phenix.autobuild data=AutoSol_run_1_/phaser_2.mtz \
  model=AutoMR_run_1_/MR.1.pdb seq_file=sequence.dat rebuild_in_place=False
</PRE>
<p>AutoBuild will now take the phases from your AutoSol run and combine them
with model-based information from your AutoMR MR solution, and will carry
out iterative density modification, model-building and refinement to rebuild
your model. </p>
<p>Note that you may wish to set <b>rebuild_in_place=True</b>, 
depending on how good your MR model is.


<P><H5>SAD dataset, reading heavy-atom sites from a PDB file 
written by <b>phenix.hyss</b></H5>
<PRE style="face=courier">phenix.autosol 11 Pb data=deriv.sca seq_file=seq.dat \
  sites_file=deriv_hyss_consensus_model.pdb 
</PRE>
This will carry out the usual structure solution process, but will read sites
from deriv_hyss_consensus_model.pdb, try both hands, and carry on from there.
If you know the hand of the substructure, 
you can fix it with <b>have_hand=True</b>.


<P><H5>MAD dataset</H5><P>
<P>The inputs for a MAD dataset need to specify f_prime and f_double_prime
for each wavelength.  You can use a parameters file &quot;mad.eff&quot; 
to input MAD data. You run it with &quot;phenix.autosol mad.eff&quot;.
Here is an example of a parameters file for a MAD dataset. You can set
many additional parameters as well (see the list at the end of this
document).

<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  sites = 2
  atom_type = Se
  wavelength {
    data = peak.sca
    lambda = .9798
    f_prime = -8.0
    f_double_prime = 4.5
  }
  wavelength {
    data = inf.sca
    lambda = .9792
    f_prime = -9.0
    f_double_prime = 1.5
  }
</PRE>

<P><H5>MAD dataset, selecting particular datasets from an MTZ file</H5><P>
This is similar to the case for running a SAD analysis,
selecting particular columns of 
data from an MTZ file.
If you have an input MTZ file with more than one anomalous dataset, you can 
use a parameters file like the one above for MAD data, but adding
information on the labels in the MTZ file that are to be chosen for
each wavelength:
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  sites = 2
  atom_type = Se
  wavelength {
    data = mad.mtz
    lambda = .9798
    f_prime = -8.0
    f_double_prime = 4.5
    labels='peak(+) SIGpeak(+) peak(-) SIGpeak(-)'

  }
  wavelength {
    data = mad.mtz 
    lambda = .9792
    f_prime = -9.0
    f_double_prime = 1.5
    labels='infl(+) SIGinfl(+) infl(-) SIGinfl(-)'
  }
}
</PRE>
This will carry out the usual structure solution, but will choose the
input peak data columns based on the label keywords.
<P>As in the SAD case, you can find out all the possible 
label strings to use by typing:
<PRE style="face=courier">phenix.autosol display_labels=w1.mtz  # display all labels for w1.mtz
</PRE>

<P><H5>SIR dataset</H5><P>
<P>The standard inputs for an SIR dataset are the native and derivative, the
sequence file, the heavy-atom type, and the number of sites, as well as
whether to use anomalous differences (or just isomorphous differences):
<PRE style="face=courier">
phenix.autosol native.data=native.sca deriv.data=deriv.sca \
   atom_type=I sites=2 inano=inano
</PRE>
This will set the heavy-atom type to Iodine, look for 2 sites, and include
anomalous differences.
<P>You can also specify many more parameters using a parameters file.
This parameters file shows some of them:
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  native {
    data = native.sca
  }
  deriv {
    data = pt.sca
    lambda = 1.4
    atom_type = Pt
    f_prime = -3.0
    f_double_prime = 3.5
    sites = 3 
  }
</PRE>

<P><H5>SAD with more than one anomalously-scattering atom </H5><P>
<P>You can tell the AutoSol wizard to look for more than one anomalously-
scattering atom. Specify one atom type (Se) in the usual way. Then specify 
any additional ones like this if you are running AutoSol from the
command line:
<PRE style="face=courier">mad_ha_add_list="Br Pt"
mad_ha_add_f_prime_list=" -7 -10"
mad_ha_add_f_double_prime_list=" 4.2 12"
</PRE>
There must be the same number of entries in each of these three keyword
lists. During phasing Phaser will try to add whichever atom types 
best fit the scattering from each new site. This option is available
for SAD phasing only and only for a single dataset (not with SAD+MIR etc).

<P><H5>MIR dataset</H5><P>
<P> It is easiest to run an MIR dataset using a parameters file such 
as &quot;mir.eff&quot; which you then run 
with &quot;phenix.autosol mir.eff&quot;.

Here is an example parameters file for MIR:
<P>
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  native {
    data = native.sca
  }
  deriv {
    data = pt.sca
    lambda = 1.4
    atom_type = Pt
  }
  deriv {
    data = ki.sca
    lambda = 1.5
    atom_type = I
  }
}
</PRE>
<P>You can enter as many derivatives as you want. If you specify a wavelength
and heavy atom type
then scattering factors are calculated from a table for that heavy-atom.
You can instead enter scattering factors with the keywords 
&quot;f_prime = -3.0 &quot;
&quot;f_double_prime = 5.0&quot; if you want.

<P><H5>SIR + SAD datasets</H5><P>
<P> A combination of SIR and SAD datasets (or of SAD+SAD or MIR+SAD+SAD or any
other combination) is easy with a parameters file.  You tell the wizard which
grouping each wavelength, native, or derivative goes with with a
keyword such as &quot;group=1&quot;.
<P>
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  native {
    group = 1
    data = native.sca
  }
  deriv {
    group = 1
    data = pt.sca
    lambda = 1.4
    atom_type = Pt
  }
  wavelength {
    group = 2
    data = w1.sca
    lambda = .9798
    atom_type = Se
    f_prime = -7.
    f_double_prime = 4.5
  }
</PRE>
<P>The SIR and SAD datasets will be solved separately (but whichever
one is solved first will use difference Fourier or anomalous difference 
Fourier's to locate sites for the other). Then phases will be combined
by addition of Hendrickson-Lattman coefficients and the combined phases
will be density modified.

      <P><H5><U>Possible Problems</U></H5><P>

<P><H5>General limitations</H5><P>

<P><H5>Specific limitations and problems</H5><P>

<P><UL><LI>The size of the asymmetric unit in the SOLVE/RESOLVE portion of 
the AutoSol wizard is limited by the memory in your computer and the
binaries used. The Wizard is supplied with regular-size ("", size=6), 
giant ("_giant", size=12), huge ("_huge", size=18) and extra_huge
("_extra_huge", size=36).  Larger-size versions can be obtained on
request.
<P><LI>The command-line version of AutoSol cannot be used for MIR or for 
combining multiple datasets.  The script and GUI versions can be used
instead for these cases.

<P><LI>The keywords &quot;cell&quot; and &quot;sg&quot; have 
been replaced with &quot;unit_cell&quot; and &quot;space_group&quot; to
make the keywords the same as in other phenix applications.

<P><LI>The keywords for running MIR and SIR and MAD datasets from 
parameter files and the command line have been changed to make the
inputs more consistent and suitable for a static GUI.

<P><LI>The AutoSol Wizard can take a maximum of 6 derivatives for MIR.

<P><LI>The AutoSol Wizard can take most settings of most space groups,
however it can only use the hexagonal setting of rhombohedral space groups 
(eg., #146 R3:H or #155 R32:H), and it cannot use space groups 114-119 (not
found in macromolecular crystallography) even in the standard setting 
due to difficulties with the use of asuset in the version of ccp4 libraries 
used in PHENIX for these settings and space groups.

</UL>

      <P><H5><U>Literature</U></H5><P>
<TABLE width=670 border=0>
  <TBODY>
  <TR>
    <TD>
      <TABLE cellSpacing=3 cellPadding=1 width="100%" align=center border=0>
        <TBODY>

        <TR id=mccoy2004>
          <TD align=left bgColor=eeeee8><FONT 
            color=000000 size=2><B>Simple algorithm for a maximum-likelihood
            SAD function.</B> A..J. McCoy, L.C. Storoni and R.J. Read. 
            <A href="http://www.iucr.org/cgi-bin/paper?ea5015">
            <I>Acta Cryst.</I>
            <B>D60</B>, 1220-1228 (2004)</A><BR></FONT></TD>
          <TD align=center bgColor=ffffff><A
            href="http://www.phenix-online.org/papers/ea5015_reprint.pdf"
            target=_main><FONT  color=000000
            size=2>[pdf]</FONT></A> </TD></TR>

        <TR id=grossekunstleve2003c>
          <TD align=left bgColor=eeeee8><FONT 
            color=000000 size=2><B>Substructure search procedures for
            macromolecular structures.</B> R.W. Grosse-Kunstleve and P.D. Adams.
            <A href="http://www.iucr.org/cgi-bin/paper?ba5048"><I>Acta
            Cryst.</I> <B>D59</B>, 1966-1973 (2003)</A><BR></FONT></TD>
          <TD align=center bgColor=ffffff><A
            href="http://www.phenix-online.org/papers/ba5048_reprint.pdf"
            target=_main><FONT  color=000000
            size=2>[pdf]</FONT></A> </TD></TR>

        <TR id=terwilliger1994a>
          <TD align=left bgColor=eeeee8><FONT 
            color=000000 size=2><B>
            MAD phasing: Bayesian estimates of F<SUB>A</SUB> </B>
            T. C. Terwilliger
            <A href= "http://www.iucr.org/cgi-bin/paper?am0003" ><I>
             Acta Cryst.  </I> <B> D50 </B>, 11-16 (1994)
            </A><BR></FONT></TD>

          <TD align=center bgColor=ffffff><A
            href= "http://www.phenix-online.org/papers/related/am0003_bayes_fa_1994.pdf"
            target=_main><FONT  color=000000
            size=2>[pdf]</FONT></A> </TD></TR>



        </TBODY>
     </TABLE>
    </TD>
  </TR>
  </TBODY>
</TABLE>

      <P><H5><U>Additional information</U></H5><P>


<!--REMARK PHENIX BODY END-->
