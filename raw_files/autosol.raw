
<!--REMARK PHENIX TITLE START  Put your title here>


<H4><U>Automated structure solution with AutoSol</U></H4>


<!--REMARK PHENIX TITLE END-->

<!--REMARK PHENIX BODY START   Put your text here. 
Anything enclosed in header html H4 H5 etc will go in the table of contents>


      <P><H5><U>Author(s)</U></H5><P>
<UL><LI>AutoSol Wizard: Tom Terwilliger
<LI>PHENIX GUI: Nathaniel Echols
<LI>HYSS: Ralf W. Grosse-Kunstleve and Paul D. Adams
<LI>Phaser: Randy J. Read, Airlie J. McCoy and Laurent C. Storoni
<LI>SOLVE:  Tom Terwilliger
<LI>RESOLVE: Tom Terwilliger
<LI>phenix.refine: Ralf W. Grosse-Kunstleve, Peter Zwart and Paul D. Adams
<LI>phenix.xtriage: Peter Zwart
</UL>

      <P><H5><U>Purpose</U></H5><P>
The AutoSol Wizard uses HYSS, SOLVE, Phaser, RESOLVE,  xtriage and 
phenix.refine to solve a structure and generate experimental phases
with the MAD, MIR, SIR, or SAD methods. 
The Wizard begins with
datafiles (.sca, .hkl, etc) containing amplitidues (or intensities)
of structure factors,
identifies heavy-atom sites, calculates phases, carries out density
modification and NCS identification, and builds and refines a 
preliminary model.

      <P><H5><U>Usage</U></H5><P>
<P>The AutoSol Wizard can be run from the PHENIX GUI, from the command-line, 
and from parameters files.  All three versions are identical except
in the way that they take commands from the user. 
See 
<a href="running-wizards.htm">
Using the PHENIX Wizards</a> 
for details of how to run a Wizard.
The command-line version will be described here, except for MIR and multiple
datasets, which can only be run with the GUI or with a parameters file.
The <a href="autosol_gui.htm">GUI is documented separately.</a>
<P>
<P><H5>How the AutoSol Wizard works</H5><P>
<P>The basic steps that the AutoSol Wizard carries out are
described below. They are: Setting up inputs,
Analyzing and scaling the data, Finding heavy-atom 
(anomalously-scattering atom) sites, Scoring of heavy-atom solutions,
Phasing, Density modification (including NCS averaging), and
Preliminary model-building and refinement. The data for structure solution
are grouped into Datasets and solutions are stored in Solution objects.

<P><H5><I>Setting up inputs</I></H5><P>
<P>The AutoSol Wizard expects the following basic information:
<P>(1) a datafile name (w1.sca or data=w1.sca)
<P>(2) a sequence file (seq.dat or seq_file=seq.dat)
<P>(3) how many sites to look for (2 or sites=2)
<P>(4) what the anomalously-scattering atom is (Se or atom_type=Se)
<P>(5) It is helpful to add the wavelength and 
  f_prime and f_double_prime for each wavelength
   or derivative that you have as well
<P> You can also specify many other parameters, including resolution,
number of sites, whether to search in a thorough or quick fashion, how
thoroughly to build a model, etc. If you have a heavy-atom solution 
from a previous run or another approach, you can read it in directly as well.
<P>Your parameters can be specified on the command-line, using a GUI, 
or by editing a parameters file (examples below).

<P><H5><I>Datafile formats in AutoSol</I></H5><P>
<P>AutoSol will accept the following formats of data:
<UL>
<LI>scalepack unmerged original index: I,SIGI</LI>
<LI>scalepack premerged: I+,SIGI+,I-,SIGI-</LI>
<LI>mtz unmerged: I, SIGI, M_ISYM</LI>
<LI>mtz premerged: I(+), SIGI(+), I(-), SIGI(-)</LI>
<LI>d*trek </LI>
<LI>CNS</LI>
</UL>
<P>The data from any of these formats will be converted to amplitudes 
(F+ , sigF+, and F-, sigF-) internally.
<P>For the best scaling results, you should supply all scalepack unmerged
original index files or all mtz unmerged files.  If all the files are 
scalepack unmerged original index or all the files are mtz unmerged
and no anisotropy correction is applied, then SOLVE local scaling will be 
applied to the data prior to merging and averaging equivalent reflections.  
In all other cases equivalent reflections will be averaged prior to scaling, 
so that the scaling may not be as effective at removing systematic errors 
due to absorption or other effects.

<P><H5><I>Sequence file format</I></H5><P>
The sequence file for autosol (and autobuild) is the one-letter code sequence
of each chain in the structure to be solved, where chains are separated by 
blank lines or lines starting with a > character. Here is a simple example
with two different chains:
<PRE style="face=courier">&lt;&lt; sequence of chain A follows. This line not required
LVLKWVMSTKYVEAGELKEGSYVVIDGEPCRVVEIEKSKTGKHGSAKARIVAVGVFDGGKRTLSLPVDAQVEVPIIEKFT

&lt;&lt; sequence of chain B follows. This line could be blank to indicate new chain
AQILSVSGDVIQLMDMRDYKTIEVPMKYVEEEAKGRLAPGAEVEVWQILDRYKIIRVKG
</PRE>
<P>
Usually the chain type (RNA, DNA, PROTEIN) is guessed from the sequence file.
You can also specify it directly with a command such as chain_type=PROTEIN.
<P>
If there are multiple copies of a chain (ncs) then you can put in a single
copy and it will be used for all of them.  If there are multiple copies of
a set of chains (A,A,A, B,B,B would be 3 copies of chains A and B) then you can
put in the unique set (A and B).  If there are different numbers of copies of
different chains, then put in the relative number of copies of each chain (for a
structure with 2 copies of A and one of B, put in A,A,B; for a structure with
4 of A and 2 B put in either (A,A,A,A,B,B) or (A,A,B).  The solvent content
will be calculated from the value of ncs_copies and the sequences and 
stoichiometry defined by your sequence file.  If you specify ncs_copies=nn
then nn will be used for ncs_copies; otherwise ncs_copies will be guessed from
the composition in your sequence file, with the contents of the asymmetric
unit being calculated as ncs_copies copies of the contents of your sequence
file.
<P>
If you have more than one type of chain (RNA, DNA, PROTEIN) then just put in the sequence for the one that is the biggest, and be sure to specify solvent_fraction=xxx so that the correct solvent fraction is used.
<P> NOTE 1: Characters such as numbers and non-printing characters in the sequence file are ignored.
<P>NOTE 2: Be sure that your sequence file does not have any blank lines in the middle of your sequence, as these are interpreted as the beginning of another chain.

<P><H5><I>Datasets and Solutions in AutoSol </I></H5><P>
<P>AutoSol breaks down the data for a structure solution into
datasets, where a dataset is a set of data that corresponds to a single
set of heavy-atom sites.  An entire MAD dataset is a single dataset.  An
MIR structure solution consists of several datasets (one for each 
native-derivative combination).  A MAD + SIR structure has one dataset
for the MAD data and a second dataset for the SIR data.  The heavy-atom
sites for each dataset are found separately (but using difference Fouriers
from any previously-solved datasets to help). In the phasing step all the
information from all datasets is merged into a single set of phases.
<P>The AutoSol wizard uses a "Solution" object to keep track of
heavy-atom solutions and the phased datasets that go with them. There are two
types of Solutions: those which consist of a single dataset (Primary
Solutions) and those that are combinations of datasets (Composite Solutions).
"Primary" Solutions have information on the datafiles that were part of the
dataset and on the heavy-atom sites for this dataset. Composite Solutions are
simply sets of Primary Solutions, with associated origin shifts.
The hand of the heavy-atom or anomalously-scattering atom substructure
is part of a Solution, so if you have two datatsets, each with two Solutions
related by inversion, then AutoSol would normally construct four different
Composite Solutions from these and score each one as described below.

<P><H5><I>Analyzing and scaling the data</I></H5><P>
<P>The AutoSol Wizard analyzes input datasets with phenix.xtriage to 
identify twinning and other conditions that may require special care. The 
data is scaled with SOLVE. For MAD data, FA values are calculated as well.

<P>Note on anisotropy corrections:

<P>The AutoSol wizard will apply an anistropy correction and B-factor
sharpening to all the raw experimental data by default 
(controlled by they keyword remove_aniso=True).  The target overall Wilson
B factor can be set with the keyword b_iso, as in  b_iso=25. By default the target
Wilson B will be 10 times the resolution of the data (e.g., if the resolution
is 3 A then b_iso=30.), or the actual Wilson B of the data, whichever is lower.
<P>If an anisotropy correction is applied then the entire AutoSol run will be
carried out with anisotropy-corrected and sharpened data.  At the very end of the
run the final model will be re-refined against the uncorrected refinement data
and this re-refined model and the uncorrected refinement data (with freeR flags)
will be written out. For the top solution this will be
 as overall_best.pdb and  overall_best_refine_data.mtz; for all other solutions the
files will be listed at the end of the log file.


<P><H5><I>Finding heavy-atom (anomalously-scattering atom) sites</I></H5><P>
<P>The AutoSol Wizard uses HYSS to find heavy-atom sites. 
The result of this step is a list of possible heavy-atom
solutions for a dataset. For SIR or SAD data,
the isomorphous or anomalous differences, respectively are used as input to
HYSS. For MAD data, the anomalous differences at each wavelength, and 
the FA estimates of complete heavy-atom structure factors from SOLVE are each
used as separate inputs to HYSS. Each heavy-atom substructure obtained from
HYSS corresponds to a potential solution. In space groups where the heavy-atom
structure can be either hand, a pair of enantiomorphic solutions is saved for
each run of HYSS. 

<P><H5><I>Running AutoSol separately in related space groups </I></H5><P>
AutoSol will check for the opposite hand of the heavy-atom solution, and
at the same time it will check for the opposite hand of your space group
(It will invert the heavy-atom solution from HYSS and invert the hand of the
space group at the same time). Therefore you do not need to run AutoSol
twice for space groups that are chiral (for example P41).  The
corresponding inverse space groups will be checked automatically (P43 ).
<p>If there are possibilities for your space group other than the inverse
hand of the space group, then
you should test them all, one at a time. 
For example if you were not able to measure
00l reflections in a hexagonal space group, your
space group  might be P6, P61, P62, P63, P64 or P65. 
In this case you  would have to run it in P6, P61 P62 and P63 (and then
P65 and P64 will be done automatically as the inverses of P61 and P62).
Normally only one of these will give a plausible solution.

<P><H5><I>Scoring of heavy-atom solutions</I></H5><P>
Potential heavy-atom solutions are scored based on a set of criteria 
(SKEW, CORR_RMS, CC_DENMOD, RFACTOR, NCS_OVERLAP,TRUNCATE, REGIONS, CONTRAST, 
FOM, FLATNESS,
described below), using either a Bayesian estimate or a
Z-score system to put all the scores on a common scale and to combine them
into a single overall score. 
<p>The overall
scoring method chosen (BAYES-CC or Z-SCORE) is determined by the
value of the keyword overall_score_method. The default is BAYES-CC.
<p>Note that for all scoring methods, the map that is being evaluated,
and the estimates of map-perfect-model correlation, refer to 
the experimental electron density map, not the density-modified map. 


<P><b>Bayesian CC scores (BAYES-CC).</b> Bayesian estimates of the quality
of experimental electron density maps are obtained using
data from a set of previously-solved datasets.
The standard scoring criteria were evaluated for 1905 potential solutions in a
set of 246 MAD, SAD, and MIR datasets.  As each dataset had previously been
solved, the correlation between the refined model and each experimental
map (CC_PERFECT) could be calculated for each solution (after offsetting
the maps to account for origin differences).
Histograms were tabulated of the number of instances
that a scoring criterion (e.g., SKEW) had various possible values, as a function
of the CC_PERFECT of the corresponding experimental map to the refined model.
These histograms yield the relative probability of measuring a particular value
of that scoring criterion (SKEW), given the value of CC_PERFECT. Using
Bayes' rule, these probabilities can be used to estimate the relative 
probabilities of values of CC_PERFECT given the value of each
scoring criterion for a particular electron density map. 
The mean estimate (BAYES-CC) is reported (multiplied x 100),
 with a +/-2SD estimate of the uncertainty in this estimate of CC_PERFECT.
<p>The BAYES-CC values are
estimated independently for each scoring criterion used, and also from all
those selected with the keyword score_type_list and not selected
with the keyword skip_score_list. 

<P><b>Z-scores (Z-SCORE).</b> 
The Z-score for one criterion for a particular solution is given by,
<PRE style="face=courier">Z= (Score - mean_random_solution_score)/(SD_of_random_solution_scores)
</PRE>
where Score is the score for this solution, mean_random_solution_score is
the mean score for a solution with randomized  phases, and 
SD_of_random_solution_scores is the standard deviation of the scores of
solutions with randomized phases.
<P>To create a total score based on Z-scores, the Z-scores for each criterion
are simply summed.

<P> The principal scoring criteria are:
<P> The skew (SKEW; 
third moment or normalized &lt;rho**3&gt;) of the density in an
electron density map is a good measure of its quality, because a random 
map has a skew of zero (density histograms look like a Gaussian), while a
good map has a very positive skew (density histograms very strong near zero,
but many points with very high density). 
This criterion is used in scoring by default.
<p>Correlation of local rms density (CORR_RMS).  The presence of contiguous
flat solvent regions in a map was detected using the correlation
coefficient of the smoothed squared electron density calculated as
described above, with the same quantity calculated using half the value of
the smoothing radius, yielding the correlation of rms density, r2RMS.  In
this way the local value of the rms density within a small local region
(typically within a radius of 3 A) is compared with the local rms density
in a larger local region (typically within a radius of 6 A). If there were
a large, contiguous solvent region and another large contiguous region
containing the macromolecule, the local rms density in the small region
would be expected to be highly correlated with the rms density in the
larger region. On the other hand, if the solvent region were broken up
into many small flat regions, then this correlation would be expected to
be smaller.
<P>Correlation of map-phased electron density map with experimentally-
phased map (CC_DENMOD). 
The statistical density modification in RESOLVE allows the 
calculation of map-based phases that are (mostly) independent of the 
experimental phases. The phase information in statistical density 
modification comes from two sources: your experimental phases and 
maximization of the agreement of the map with expectations (such as
a flat solvent region). Normally the 
phase probabilities from these two sources are merged together, yielding 
your density-modified phases. This score is calculated based on the 
correlation of the phase information from these two sources before
combining them, and is a good indication of the quality of the
experimental phases. 
This criterion is used in scoring by default.
<P> The R-factor for density modification (RFACTOR). Statistical 
density modification provides an estimate of structure factors that is
(mostly) independent of the measured structure factors, so the R-factor
between FC and Fobs is a good measure of the quality of experimental phases.
This criterion is used in scoring by default.
<P> Non-crystallographic symmetry (NCS_OVERLAP).  The presence of 
NCS in a map is a nearly-positive indication that the map is good, or has
some correct features. The AutoSol Wizard uses symmetry in heavy-atom
sites to suggest NCS, and RESOLVE identifies the actual correlation of
NCS-related density for the NCS overlap score. 
This score is used by default if NCS is present in the Z-score method of 
scoring. 
<P>Figure of merit (FOM). The figure of merit of phasing is a good 
indicator of the internal consistency of a solution. This score is not
normalized by the SD of randomized phase sets (as that has no meaning; rather
a standard SD=0.05 is used).
This score is used by default if NCS is present in the Z-score method of 
scoring and in the Bayesian CC estimate method.
<P>Map correlation after truncation (TRUNCATION). 
Dummy atoms (the same number as
estimated non-hydrogen atoms in the structure) are placed in positions
of high density of the map, and a new map is calculated based on these
atomic positions.  The correlation of these maps is calculated
after adjusting an overall B-value for the dummy atoms to maximize the
correlation.   A good map will show a high correlation of these maps.
This score is by default not used.
<P> Number of contiguous regions per 100 A**3 comprising top 5% of density
in map (REGIONS).  
The top 5% of points in the map are marked, and the number of 
contiguous regions that result are counted, and divided by the volume
 of the asymmetric unit, then multiplied by 100. A good map will have just 
a few contiguous regions at a high contour level, a poor map will have many
isolated peaks.
This score is by default not used.

<P> Contrast, or standard deviation of local rms density (CONTRAST). 
The local rms
density in the map is calculated using a smoothing radius of 3 times
the high-resolution cutoff (or 6 A, if less than 6A). 
Then the standard deviation
of the local rms, normalized to the mean value of the local rms, is
reported. This criteria will be high if there are regions of high local
rms (the macromolecule) and separate regions of low local rms (the solvent)
and low if the map is random. 
This score is by default not used.

<P><H5><I>Phasing</I></H5><P>
<P>The AutoSol Wizard uses Phaser to calculate experimental phases from
SAD data, and SOLVE to calculate phases from MIR, MAD, and multiple-dataset
cases.
<P><H5><I>Density modification (including NCS averaging)</I></H5><P>
<P>The AutoSol Wizard uses RESOLVE to carry out density modification. It 
identifies NCS from symmetries in heavy-atom sites with RESOLVE and applies
this NCS if it is present in the electron density map.
<P><H5><I>Preliminary model-building and refinement</I></H5><P>
<P>The AutoSol Wizard carries out one cycle of model-building and
refinement after obtaining density-modified phases. The model-building is done
with RESOLVE.  The refinement is carried out with phenix.refine.

<P><H5><I>Resolution limits in AutoSol</I></H5><P>
<P>There are several resolution limits used in AutoSol. You can leave them
all to default, or you can set any of them individually. Here is a list of
these limits and how their default values are set:
<TABLE width=670 border=1>
 <TBODY>
  <TR>
    <TD>Name</TD>
    <TD>Description</TD>
    <TD>How default value is set </TD>
  </TR>
  <TR>
    <TD>resolution</TD>
    <TD>Overall resolution for a dataset</TD>
    <TD>Highest resolution for any datafile in this dataset. For
       multiple datasets, the highest resolution for any dataset </TD>
  </TR>
  <TR>
    <TD>refinement_resolution</TD>
    <TD>Resolution for refinement</TD>
    <TD>value of &quot;resolution&quot;</TD>
  </TR>
  <TR>
    <TD>resolution_build</TD>
    <TD>Resolution for model-building</TD>
    <TD>value of &quot;resolution&quot;</TD>
  </TR>
  <TR>
    <TD>res_phase</TD>
    <TD>Resolution for phasing for a dataset</TD>
    <TD>If phase_full_resolution=True then 
            use value of &quot;resolution&quot;. 
         Otherwise, use value of &quot;recommended_resolution&quot;
         based on analysis of signal-to-noise in dataset.</TD>
  </TR>
  <TR>
    <TD>res_eval</TD>
    <TD>Resolution for evaluation of solution quality</TD>
    <TD>value of &quot;resolution&quot; or 2.5 A, whichever is
        lower resolution.  </TD>
  </TR>
 </TBODY>
</TABLE>

<P><H5><I>Output files from AutoSol</I></H5><P>
<P>When you run AutoSol the output files will be in a subdirectory with
your run number: 
<PRE style="face=courier">AutoSol_run_1_/
</PRE>
<P>The key output files that are produced are:

<P><UL><LI>A log file describing everything in the run and the files produced:
<PRE style="face=courier">AutoSol_run_1_1.log # overall log file
</PRE>

<P><LI>A summary file listing the results of the run and 
the other files produced:
<PRE style="face=courier">AutoSol_summary.dat  # overall summary
</PRE>

<P><LI>A warnings file listing any warnings about the run
<PRE style="face=courier">AutoSol_warnings.dat  # any warnings
</PRE>

<P><LI>Density-modified map coefficients 
(NOTE: These files be aniso-corrected and sharpened if remove_aniso=True)
<PRE style="face=courier">overall_best_denmod_map_coeffs.mtz # map coefficients (density modified phases)
</PRE>

<P><LI>Current preliminary model
<PRE style="face=courier"> overall_best.pdb # model produced for top solution
</PRE>
NOTE: If there are multiple chains or multiple ncs copies, each chain will
be given its own chainID (A B C D...).  Segments that are not assigned to a 
chain are given a separate chainID and are given a segid of "UNK" to indicate
that their assignment is unknown.  The chainID for solvent molecules is
normally S, and the chainID for heavy-atoms is normally Z.

<P><LI>An mtz file for use in refinement
NOTE 1 : not aniso corrected and not sharpened.
NOTE 2:  Two sets of HL coefficients may be present.  Normally use HLA HLB etc .  However, if
you supplied a model with input_partpdb_file=my_model.pdb then use instead HLanomA HLanomB etc.
The reason is that HL coeffs contain phase information from my_model.pdb in this case and you
do not want that information passed to your refinement program.

<PRE style="face=courier"> overall_best_refine_data.mtz # F Sigma HL coeffs, freeR-flags for refinement
</PRE>
NOTE: if this is a SAD or MAD dataset then the overall_best_refine_data.mtz file will normally have
your original anomalous data.For MAD data this will be from the wavelength of data with the 
highest-resolution data present.

<P><LI>Heavy atom sites in PDB format
<PRE style="face=courier"> overall_best_ha_pdb.pdb # ha file for top solution
</PRE>

<P><LI>NCS information  (if any)
<PRE style="face=courier">overall_best_ncs_file.ncs_spec # NCS information for top solution
</PRE>

<P><LI>Experimental phases and HL coefficients 
(NOTE: These files are aniso-corrected and sharpened if remove_aniso=True)
<PRE style="face=courier">overall_best_hklout_phased.mtz # phases and HL coeffs for top solution
</PRE>

<P><LI>Log file for experimental phasing
<PRE style="face=courier">overall_best_log_phased.log # experimental phasing log file for top solution
</PRE>

<P><LI>Log file for scaling
<PRE style="face=courier">overall_best_log_phased.log # experimental phasing log file for top solution
</PRE>

<P><LI>Log file for heavy-atom substructure search
<PRE style="face=courier">overall_best_log_hyss.log # ha search log file for top solution
</PRE>

</UL>

<P><H5>How to run the AutoSol Wizard</H5><P>
<P>  Running the AutoSol Wizard is easy.  From the command-line you can 
type:
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se f_prime=-8 f_double_prime=4.5
</PRE>
<P>The AutoSol Wizard will assume that w1.sca is a datafile (because it
ends in .sca and is a file) and that seq.dat is a sequence file, that there
are 2 heavy-atom sites, and that the heavy-atom is Se. The f_prime and
f_double_prime values are set explicitly
<P> You can also specify each of these things directly:
<PRE style="face=courier">phenix.autosol data=w1.sca seq_file=seq.dat sites=2 \
   atom_type=Se f_prime=-8 f_double_prime=4.5
</PRE>
<P>You can specify many more parameters as well. See  the list of
keywords, defaults and descriptions at the end of this page and
also general information about running Wizards at
<a href="running-wizards.htm">
Using the PHENIX Wizards</a> 
for how to do this.  Some of the most common parameters are:
<PRE style="face=courier">sites=3     # 3 sites
sites_file=sites.pdb  # ha sites in PDB or fractional xyz format
atom_type=Se   # Se is the heavy-atom
seq_file=seq.dat   # sequence file (1-aa code, separate chains with >>>>)
quick=True  # try to find sites quickly
data=w1.sca  # input datafile
lambda=0.9798  # wavelength for SAD
</PRE>

<P><H5>Running from a parameters file</H5><P>
You can run phenix.autosol from a parameters file. This is often convenient
because you can generate a default one with:
<PRE style="face=courier">phenix.autosol --show_defaults > my_autosol.eff
</PRE>
and then you can just edit this file to match your needs and run it with:
<PRE style="face=courier">phenix.autosol  my_autosol.eff
</PRE>
NOTE: the autosol parameters file my_autosol.eff will have just one 
blank native, derivative, and wavelength. You can cut and paste them to 
put in as many as you want to have.

<P>

<P><H5><I>Model viewing during model-building with the Coot-PHENIX interface 
</I></H5><P>
<P>The AutoSol Wizard allows you to view the current best model that is
produced by the automated model-building process.  This capability is 
identical to the view/edit model procedure available in the 
<A href="autobuild.htm"> AutoBuild </A>
Wizard. Normally you would use it just to view the model in AutoSol, and
to view and edit a model in 
<A href="autobuild.htm"> AutoBuild </A> .
<p>The PHENIX-Coot interface is accessible via the command-line.
When a model has been produced by the AutoSol Wizard, 
you can open a new window and
type:
<PRE style="face=courier">phenix.autobuild coot </PRE>
which will start Coot with your current map and model.
<p>When Coot has been loaded, your map and model will be displayed along with
a <b>PHENIX-Coot Interface</b> window. If you want, 
you can edit your model and then save 
it, giving it back to PHENIX with the button labelled something like
<b>Save model as COMM/overall_best_coot_7.pdb</b>. This button creates the
indicated file and also tells PHENIX to look for this file and to try and
include the contents of the model in the building process. In AutoSol, only
the main-chain atoms of the model you save are considered, and
the side-chains are ignored.  Ligands 
and solvent in the model are ignored as well.
<p>As the AutoSol Wizard continues to build new models and create new maps,
you can update in the PHENIX-Coot Interface to the current best model and
map with the button <b>Update with current files from PHENIX</b>.
      <P><H5><U>Examples</U></H5><P>
<P><H5>SAD dataset</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se lambda=0.9798
</PRE>
The sequence file is used to estimate the solvent content of the
crystal and for model-building.  The wavelength (lambda) is used to look up
values for f_prime and f_double_prime from a table, but if measured values
are available from a fluorescence scan, these should be given in addition to
the wavelength.

<P><H5>SAD dataset specifying solvent fraction</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se lambda=0.9798 \
    solvent_fraction=0.45
</PRE>
This will force the solvent fraction to be 0.45.  This illustrates a
general feature of the Wizards: they will try to estimate values of
parameters, but if you input them directly, they will use your input
values.

<P><H5>SAD dataset without model-building</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se lambda=0.9798 \
    build=False
</PRE>
This will carry out the usual structure solution, but will skip model-building

<P><H5>SAD dataset, building RNA instead of protein</H5><P>
<PRE style="face=courier">phenix.autosol w1.sca seq.dat 2 Se lambda=0.9798 \
    chain_type=RNA
</PRE>
This will carry out the usual structure solution, but will build an RNA
chain.  For DNA, specify chain_type=DNA.  You can only build one type of
chain at a time in the AutoSol Wizard. To build protein and DNA, use 
the <A href="autobuild.htm"> AutoBuild </A>
Wizard and run it first with chain_type=PROTEIN, then
run it again specifying the protein 
model as input_lig_file_list=proteinmodel.pdb
and with chain_type=DNA.

<P><H5>SAD dataset, selecting a particular dataset from an MTZ file</H5><P>
If you have an input MTZ file with more than one anomalous dataset, you can 
type something like:
<PRE style="face=courier">phenix.autosol w1.mtz seq.dat 2 Se lambda=0.9798 \
labels='F+ SIGF+ F- SIGF-'
</PRE>
This will carry out the usual structure solution, but will choose the
input data columns based on the  labels: 'F+ SIGF+ F- SIGF-'

NOTE: to specify anomalous data with F+ SIGF+ F- SIGF- like this, 
these 4 columns must be adjacent to each other in the MTZ file with
no other columns in between. 
FURTHER NOTE: to instead use a FAVG SIGFAVG DANO SIGDANO
array in AutoSol, the data file or an input refinement file MUST also contain
a separate array for FP SIGFP or I SIGI or equivalent. This is because
FAVG DANO arrays are ONLY allowed as anomalous information, not as amplitudes
or intensities. You can use F+ SIGF+ F- SIGF- arrays as a source of both 
anomalous differences and amplitudes if you want, however.
<P>
 If you run the
AutoSol Wizard with SAD data and an MTZ file containing more than one
anomalous dataset and don't tell it which one to use, 
all possible values of labels are 
printed out for you so that you can just paste the one you want in.
<P>You can also find out all the possible label strings to use by typing:
<PRE style="face=courier">phenix.autosol display_labels=w1.mtz  # display all labels for w1.mtz
</PRE>

<P><H5>MRSAD -- SAD dataset with an MR model; Phaser SAD phasing including the model</H5>

<p>If you are carrying out SAD phasing with Phaser, you can carry out a combination of 
molecular replacement phasing and SAD phasing (MRSAD) by adding
a single new keyword to your AutoSol run:
</p><PRE style="face=courier">input_partpdb_file=MR.pdb
</PRE>
In this case the MR.pdb file will be used as a partial model in a maximum-likelihood
SAD phasing calculation with Phaser to calculate phases and identify
sites in Phaser, and the combined MR+SAD phases will be written out.
<p>NOTE: At the moment the AutoBuild Wizard is not equipped to use these combined
phases optimally in iterative model-building, density modification
and refinement, because they contain both experimental phase information and model
information. It is therefore possible that the resulting phases are biased by your 
MR model, and that this bias will not go away during iterative model-building because it
is continually fed back in.

<P><H5>Using an MR model to find sites and as a source of phase information (method #2 for MRSAD)</H5>

You can also combine MR information with SAD phases (see J. P. Schuermann and 
J. J. Tanner Acta Cryst. (2003). D59, 1731-1736 ) in PHENIX
by running the three wizards AutoMR, AutoSol, and AutoBuild one after the
other.  This method does not use the partial model and the
anomalous information in the SAD dataset simultaneously as the above Phaser 
maximum-likelihood method does. On the other hand, the phases obtained 
in this method are independent of the model, so that combining them 
afterwards does not introduce model bias. (It is not yet clear which is the better
approach, so you may wish to try both.) Additionally, this approach can be used with
any method for phasing.

<p>Here is a set of three simple commands to do this:
<p>First run <b>AutoMR</b> to find the molecular replacement solution, but 
don't rebuild it yet:
<PRE style="face=courier">phenix.automr gene-5.pdb peak.sca copies=1 \
  RMS=1.5 mass=9800 rebuild_after_mr=False
</PRE>
<p>Now your MR solution is in <b>AutoMR_run_1_/MR.1.pdb</b> and 
phases are in <b>AutoMR_run_1_/MR.1.mtz</b>.  Use these phases as input
to AutoSol, along with some weak SAD data, still not building any new models:
<PRE style="face=courier"> phenix.autosol data=peak.sca \
 input_phase_file=AutoMR_run_1_/MR.1.mtz input_phase_labels="F PHIC FOM"   \
seq_file=sequence.dat build=False
</PRE>
<p> note that we have specified the data columns for F PHI and FOM in the
input_phase_file. For input_phase_file you must specify all three of these (if 
you leave out FOM it will set it to zero). </p>
<p>AutoSol will write an MTZ file with experimental phases to <b>phaser_xx.mtz</b> where xx depends on how many solutions are considered during the run.  The
next command for running AutoBuild  you will need to edit depending 
on the value of xx:
<PRE style="face=courier"> phenix.autobuild data=AutoSol_run_1_/phaser_2.mtz \
  model=AutoMR_run_1_/MR.1.pdb seq_file=sequence.dat rebuild_in_place=False
</PRE>
<p>AutoBuild will now take the phases from your AutoSol run and combine them
with model-based information from your AutoMR MR solution, and will carry
out iterative density modification, model-building and refinement to rebuild
your model. </p>
<p>Note that you may wish to set <b>rebuild_in_place=True</b>, 
depending on how good your MR model is.

<P><H5>Using placed density from a Phaser MR density search to find sites and as a source of phase information (method #3 for MRSAD)</H5>
You can carry out a procedure almost like using input_partpdb_file except that the MR model comes as a set of map coefficients for density that has been placed by Phaser.  In this case the keyword to use is input_part_map_coeffs_file instead of input_partpdb_file.  You can also identify the labels with input_part_map_coeffs_labels.


<P><H5>SAD dataset, reading heavy-atom sites from a PDB file 
written by <b>phenix.hyss</b></H5>
<PRE style="face=courier">phenix.autosol 11 Pb data=deriv.sca seq_file=seq.dat \
  sites_file=deriv_hyss_consensus_model.pdb lambda=0.95
</PRE>
This will carry out the usual structure solution process, but will read sites
from deriv_hyss_consensus_model.pdb, try both hands, and carry on from there.
If you know the hand of the substructure, 
you can fix it with <b>have_hand=True</b>.


<P><H5>MAD dataset</H5><P>
<P>The inputs for a MAD dataset need to specify f_prime and f_double_prime
for each wavelength.  You can use a parameters file &quot;mad.eff&quot; 
to input MAD data. You run it with &quot;phenix.autosol mad.eff&quot;.
Here is an example of a parameters file for a MAD dataset. You can set
many additional parameters as well (see the list at the end of this
document).

<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  sites = 2
  atom_type = Se
  wavelength {
    data = peak.sca
    lambda = .9798
    f_prime = -8.0
    f_double_prime = 4.5
  }
  wavelength {
    data = inf.sca
    lambda = .9792
    f_prime = -9.0
    f_double_prime = 1.5
  }
}
</PRE>

<P><H5>MAD dataset, selecting particular datasets from an MTZ file</H5><P>
This is similar to the case for running a SAD analysis,
selecting particular columns of 
data from an MTZ file.
If you have an input MTZ file with more than one anomalous dataset, you can 
use a parameters file like the one above for MAD data, but adding
information on the labels in the MTZ file that are to be chosen for
each wavelength:
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  sites = 2
  atom_type = Se
  wavelength {
    data = mad.mtz
    lambda = .9798
    f_prime = -8.0
    f_double_prime = 4.5
    labels='peak(+) SIGpeak(+) peak(-) SIGpeak(-)'

  }
  wavelength {
    data = mad.mtz 
    lambda = .9792
    f_prime = -9.0
    f_double_prime = 1.5
    labels='infl(+) SIGinfl(+) infl(-) SIGinfl(-)'
  }
}
</PRE>
This will carry out the usual structure solution, but will choose the
input peak data columns based on the label keywords.
<P>As in the SAD case, you can find out all the possible 
label strings to use by typing:
<PRE style="face=courier">phenix.autosol display_labels=w1.mtz  # display all labels for w1.mtz
</PRE>

<P><H5>SIR dataset</H5><P>
<P>The standard inputs for an SIR dataset are the native and derivative, the
sequence file, the heavy-atom type, and the number of sites, as well as
whether to use anomalous differences (or just isomorphous differences):
<PRE style="face=courier">
phenix.autosol native.data=native.sca deriv.data=deriv.sca \
   atom_type=I sites=2 inano=inano
</PRE>
This will set the heavy-atom type to Iodine, look for 2 sites, and include
anomalous differences.
<P>You can also specify many more parameters using a parameters file.
This parameters file shows some of them:
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  native {
    data = native.sca
  }
  deriv {
    data = pt.sca
    lambda = 1.4
    atom_type = Pt
    f_prime = -3.0
    f_double_prime = 3.5
    sites = 3 
  }
}
</PRE>

<P><H5>SAD with more than one anomalously-scattering atom </H5><P>
<P>You can tell the AutoSol wizard to look for more than one anomalously-
scattering atom. Specify one atom type (Se) in the usual way. Then specify 
any additional ones like this if you are running AutoSol from the
command line:
<PRE style="face=courier">
mad_ha_add_list="Br Pt"
</PRE>
Optionally, you can add f_prime and f_double_prime values for the additional 
atom types with commands like
<PRE style="face=courier">
mad_ha_add_f_prime_list=" -7 -10"
mad_ha_add_f_double_prime_list=" 4.2 12"
</PRE>
but the values from table lookup should be fine.  Note that
there must be the same number of entries in each of these three keyword
lists, if given. During phasing Phaser will try to add whichever atom types 
best fit the scattering from each new site. This option is available
for SAD phasing only and only for a single dataset (not with SAD+MIR etc).

<P><H5>MIR dataset</H5><P>
<P> It is easiest to run an MIR dataset using a parameters file such 
as &quot;mir.eff&quot; which you then run 
with &quot;phenix.autosol mir.eff&quot;.

Here is an example parameters file for MIR:
<P>
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  native {
    data = native.sca
  }
  deriv {
    data = pt.sca
    lambda = 1.4
    atom_type = Pt
  }
  deriv {
    data = ki.sca
    lambda = 1.5
    atom_type = I
  }
}
</PRE>
<P>You can enter as many derivatives as you want. If you specify a wavelength
and heavy atom type
then scattering factors are calculated from a table for that heavy-atom.
You can instead enter scattering factors with the keywords 
&quot;f_prime = -3.0 &quot;
&quot;f_double_prime = 5.0&quot; if you want.

<P><H5>SIR + SAD datasets</H5><P>
<P> A combination of SIR and SAD datasets (or of SAD+SAD or MIR+SAD+SAD or any
other combination) is easy with a parameters file.  You tell the wizard which
grouping each wavelength, native, or derivative goes with with a
keyword such as &quot;group=1&quot;.
<P>
<PRE style="face=courier">
autosol {
  seq_file = seq.dat
  native {
    group = 1
    data = native.sca
  }
  deriv {
    group = 1
    data = pt.sca
    lambda = 1.4
    atom_type = Pt
  }
  wavelength {
    group = 2
    data = w1.sca
    lambda = .9798
    atom_type = Se
    f_prime = -7.
    f_double_prime = 4.5
  }
}
</PRE>
<P>The SIR and SAD datasets will be solved separately (but whichever
one is solved first will use difference Fourier or anomalous difference 
Fourier's to locate sites for the other). Then phases will be combined
by addition of Hendrickson-Lattman coefficients and the combined phases
will be density modified.

<P><H5>AutoSol with an extremely weak signal</H5><P>
<P>You can use AutoSol in cases with a very weak anomalous signal. The big
challenges in this kind of situation are finding the anomalously-scattering
atoms and density modification. For finding the sites, you may need to
try everything available, including running HySS with various resolution
cutoffs for the data and trying other software for finding sites as well.
You may then want to start with the sites found using one of these
approaches and provide those to phenix.autosol with a command like,
<PRE style="face=courier"> sites_file=my_sites.pdb
</PRE>
<P>For the density modification step, there is a keyword (extreme_dm)
 that may be helpful in cases where the starting phases are very poor.
This keyword works with the keyword fom_for_extreme_dm to determine 
whether a set of defaults for density modification with weak phases is
appropriate.  If so, a very large radius for identification of the 
solvent boundary (20 A) is used, and the number of density modification
cycles is reduced. This can make a big difference in getting started
with density modification in such a case.

<P><H5>AutoSol with a cluster compound</H5><P>
You can run SAD phasing in AutoSol with a cluster compound (not MIR or MAD
yet).  Normally you should supply a PDB file with an example of the cluster with the keyword cluster_pdb_file=my_cluster_compound and a unique residue name XX (yes, XX).  Set the keyword atom_type=XX as well .  If your cluster is Ta6Br12 then you can simply put atom_type=TX and  skip the cluster_pdb_file. For MAD/MIR, cluster compounds are not currently supported. Instead just use a standard atom.


      <P><H5><U>Possible Problems</U></H5><P>

<P><H5>General limitations</H5><P>

<P><H5>Specific limitations and problems</H5><P>

<P><UL><LI>The size of the asymmetric unit in the SOLVE/RESOLVE portion of 
the AutoSol wizard is limited by the memory in your computer and the
binaries used. The Wizard is supplied with regular-size ("", size=6), 
giant ("_giant", size=12), huge ("_huge", size=18) and extra_huge
("_extra_huge", size=36).  Larger-size versions can be obtained on
request.

<P><LI>The keywords &quot;cell&quot; and &quot;sg&quot; have 
been replaced with &quot;unit_cell&quot; and &quot;space_group&quot; to
make the keywords the same as in other phenix applications.

<P><LI>The keywords for running MIR and SIR and MAD datasets from 
parameter files and the command line have been changed to make the
inputs more consistent and suitable for a static GUI.

<P><LI>The AutoSol Wizard can take a maximum of 6 derivatives for MIR.

<P><LI>The AutoSol Wizard can take most settings of most space groups,
however it can only use the hexagonal setting of rhombohedral space groups 
(eg., #146 R3:H or #155 R32:H), and it cannot use space groups 114-119 (not
found in macromolecular crystallography) even in the standard setting 
due to difficulties with the use of asuset in the version of ccp4 libraries 
used in PHENIX for these settings and space groups.

</UL>

      <P><H5><U>Literature</U></H5><P>
<TABLE width=670 border=0>
  <TBODY>
  <TR>
    <TD>
      <TABLE cellSpacing=3 cellPadding=1 width="100%" align=center border=0>
        <TBODY>

        <TR id=mccoy2004>
          <TD align=left bgColor=eeeee8><FONT 
            color=000000 size=2><B>Simple algorithm for a maximum-likelihood
            SAD function.</B> A..J. McCoy, L.C. Storoni and R.J. Read. 
            <A href="http://www.iucr.org/cgi-bin/paper?ea5015">
            <I>Acta Cryst.</I>
            <B>D60</B>, 1220-1228 (2004)</A><BR></FONT></TD>
          <TD align=center bgColor=ffffff><A
            href="http://www.phenix-online.org/papers/ea5015_reprint.pdf"
            target=_main><FONT  color=000000
            size=2>[pdf]</FONT></A> </TD></TR>

        <TR id=grossekunstleve2003c>
          <TD align=left bgColor=eeeee8><FONT 
            color=000000 size=2><B>Substructure search procedures for
            macromolecular structures.</B> R.W. Grosse-Kunstleve and P.D. Adams.
            <A href="http://www.iucr.org/cgi-bin/paper?ba5048"><I>Acta
            Cryst.</I> <B>D59</B>, 1966-1973 (2003)</A><BR></FONT></TD>
          <TD align=center bgColor=ffffff><A
            href="http://www.phenix-online.org/papers/ba5048_reprint.pdf"
            target=_main><FONT  color=000000
            size=2>[pdf]</FONT></A> </TD></TR>

        <TR id=terwilliger1994a>
          <TD align=left bgColor=eeeee8><FONT 
            color=000000 size=2><B>
            MAD phasing: Bayesian estimates of F<SUB>A</SUB> </B>
            T. C. Terwilliger
            <A href= "http://www.iucr.org/cgi-bin/paper?am0003" ><I>
             Acta Cryst.  </I> <B> D50 </B>, 11-16 (1994)
            </A><BR></FONT></TD>

          <TD align=center bgColor=ffffff><A
            href= "http://www.phenix-online.org/papers/related/am0003_bayes_fa_1994.pdf"
            target=_main><FONT  color=000000
            size=2>[pdf]</FONT></A> </TD></TR>

      <TR id="terwilliger2009a">
        <TD ALIGN=LEFT BGCOLOR="#eeeee8">
          <FONT FACE="Verdana,Helvetica,Arial" COLOR="#000000">
          <b>Decision-making in structure solution using Bayesian estimates 
   of map quality: the PHENIX AutoSol wizard.</b>
          T. C. Terwilliger, P. D. Adams, R. J. Read, A. J. McCoy, N. W. Moriart
y, R. W. Grosse-Kunstleve, P. V. Afonine, P. H. Zwart and L.-W. Hung          <a href="http://www.iucr.org/cgi-bin/paper?ea5095">          <i>Acta Cryst.</i> <b>D65</b>, 582-601 (2009)</a><br>

          </FONT>
        </TD>
        <TD ALIGN=CENTER BGCOLOR="#ffffff">
          <A HREF="http://www.phenix-online.org/papers/ea5095_reprint.pdf" TARGE
T="_main"><FONT FACE="Verdana,Helvetica,Arial" COLOR="#000000">[pdf]</FONT></A> 
 
        </TD>
      </TR>


        </TBODY>
     </TABLE>
    </TD>
  </TR>
  </TBODY>
</TABLE>

      <P><H5><U>Additional information</U></H5><P>


<!--REMARK PHENIX BODY END-->
