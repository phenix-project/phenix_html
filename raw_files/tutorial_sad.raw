
<!--REMARK PHENIX TITLE START  Put your title here-->


<p><H5><U>Tutorial 1: Solving a structure with SAD data</U></H5></p><p>


<!--REMARK PHENIX TITLE END-->

<!--REMARK PHENIX BODY START   Put your text here. 
Anything enclosed in header html H5 H5 etc will go in the table of contents-->


<p><H5><U>Introduction</U></H5></p><p>

This tutorial will use some very good SAD data (peak wavelength from 
an IF5A dataset diffracting to 1.7 A) as an example of how to solve a
SAD dataset with AutoSol. It is designed to be read all the way through,
giving pointers for you along the way. Once you have read it all and run
the example data and looked at the output files, you will be in 
a good position to run your own data through AutoSol.

</p><p><H5><U>Setting up to run PHENIX</U></H5></p><p>
If PHENIX is already installed and your environment is all set, then
if you type:
</p><PRE style="face=courier">echo $PHENIX</PRE>
then you should get back something like this:
</p><PRE style="face=courier">/xtal//phenix-1.3b</PRE>
If instead you get:
</p><PRE style="face=courier">PHENIX: undefined variable</PRE>
then you need to set up your PHENIX environment. See the
<a href="install.htm" >PHENIX installation</a> page for 
details of how to do this. 
If you are using the C-shell environment (<b>csh</b>) then all you will need 
to do is add one line to your <b>.cshrc</b> (or equivalent) file that
looks like this:
</p><PRE style="face=courier">source /xtal/phenix-1.3b/phenix_env</PRE>
(except that the path in this statement will be where <b>your</b> PHENIX is
installed). Then the next time you log in <b>$PHENIX</b> will be defined.




</p><p><H5><U>Running the demo p9 data with AutoSol</U></H5></p><p>
To run AutoSol on the demo p9 data, make yourself a <b>tutorials</b>
directory and <b>cd</b> into that directory:
</p><PRE style="face=courier">mkdir tutorials
cd tutorials </PRE>
Now  type the phenix command:
</p><PRE style="face=courier">phenix.run_example --help </PRE>
to list the available examples.  Choosing <b>p9-sad</b> for this
tutorial, you can now use the phenix command:
</p><PRE style="face=courier">phenix.run_example p9-sad </PRE>
to solve the p9 structure with AutoSol. This command will
copy the directory <b>$PHENIX/examples/p9-sad</b>
to your current directory (<b>tutorials</b>) 
and call it <b>tutorials/p9-sad/</b>
. Then it will run AutoSol using the command file 
<b>run.csh</b> 
that is present in this <b>tutorials/p9-sad/</b> directory.
</p><p>This command file <b>run.csh</b> is simple. It says:

</p><PRE style="face=courier">
#!/bin/csh
phenix.autosol  seq_file=seq.dat sites=4 atom_type=Se  data=p9_se_w2.sca  \
sg=&quot;I4&quot; cell=&quot;113.949 113.949  32.474 90.000  90.000  90.00&quot;  \
resolution=2.4 thoroughness=quick
</PRE>
The first line (<b>#!/bin/csh</b>) tells the system to interpret the remainder of the text in
the file using the C-shell (<b>csh</b>).</p><p>
The command <b>phenix.autosol</b> runs the 
command-line version of AutoSol (see 
<a href="autosol.htm">Automated Structure Solution using AutoSol</a> for
all the details about AutoSol including a full list of keywords).
The arguments on the command line 
tell AutoSol about the sequence file  
(<b>seq_file=seq.dat</b>), the number of
sites to look for (<b>sites=4</b>), 
and the atom type (<b>atom_type=Se</b>).  (Note that each of these is
specified with an <b>=</b> sign, and that there are no spaces
around the <b>=</b> sign.)
The Phaser
heavy-atom refinement and model completion algorithm used in the AutoSol
SAD phasing will add additional sites if warranted. 
<p>Note the backslash &quot;\&quot; at the end of some of the lines in the
<b>phenix.autosol</b> command. This tells the C-shell (which interprets
everything in this file) that the next line is a continuation of the
current line. There must be no characters (not even a space) after
the backslash for this to work.</p>
</p><p>The SAD data to be used
to solve the structure is in the datafile
<b>p9_se_w2.sca</b>.  This datafile is in
Scalepack unmerged format, which means that there may be multiple 
instances of each reflection and the cell parameters are not in the file,
so we need to provide the cell parameters with the command,
<b> cell=&quot;113.949 113.949  32.474 90.000  90.000  90.00&quot;</b>.  
(Note that the
cell parameters are surrounded by quotation marks. That tells
the parser that these are all together.)
In this example, the space group in the 
<b>p9_se_w2.sca</b> file is 
<b>I41</b>, but the correct space group is
<b>I4</b>,  so we need to tell AutoSol the
correct space group with 
<b>sg=&quot;I4&quot;</b>.
</p><p>The resolution of the data in 
<b>p9_se_w2.sca</b>  is to 1.74 A, but in this
example we would like to solve the structure quickly, so we have cut the
resolution back with the commands
<b>resolution=2.4</b> and
<b>thoroughness=quick</b>.  
The <b>quick</b> command sets several defaults
to give a less comprehensive search for heavy-atom sites and a less thorough
model-building than if you use  the default of
<b>thoroughness=thorough</b>.  
<p>Although the <b>phenix.run_example p9-sad</b> command has just run
AutoSol from a script (<b>run.csh</b>), you can run AutoSol yourself from
the command line with the same <b>phenix.autosol seq_file= ...</b> command.
You can also run AutoSol from a GUI, or by putting commands in another
type of script file. All these possibilities are described in 
<a href="running-wizards.htm">
Running a Wizard from a GUI, the command-line, or a script</a>.
</p>

</p><p><H5>Where are my files?</H5></p><p>
Once you have started AutoSol or another Wizard, an <b>output directory</b> will
be created in your current (working) directory. The first time you run
AutoSol in this directory, this <b>output directory</b> will be
called <b>AutoSol_run_1_</b> (or <b>AutoSol_run_1_/</b>, where the slash at
the end just indicates that this is a directory).  All of the output
from run <b>1</b> of AutoSol will be in this directory. If you run 
AutoSol again, a new subdirectory called <b>AutoSol_run_2_</b> will be created.
<p>Inside the directory <b>AutoSol_run_1_</b> there will be one or more
temporary directories such as <b>TEMP0</b> created while the Wizard is
running. The files in this temporary directory may be useful sometimes in
figuring out what the Wizard is doing (or not doing!). By default these
directories are emptied when the Wizard finishes (but you can keep their
contents with the command <b>clean_up=False</b> if you want.)
</p>

</p><p><H5>What parameters did I use?</H5></p><p>
Once the AutoSol wizard has started (when run from the command line), a 
<b>parameters file</b>
called <b>autosol.eff</b> will be created in your <b>output directory</b>
(e.g., <b>AutoSol_run_1_/autosol.eff</b>).  This <b>parameters file</b>
has a header that says what command you used to run AutoSol, and it contains
all the starting values of all parameters for this run (including the 
defaults for all the parameters that you did not set).
</p>
The <b>autosol.eff</b> file is good for more than just looking at the
values of parameters, though. If you copy this file to a new one
(for example <b>autosol_hires.eff</b>) and edit it to change the values of
some of the parameters (<b>resolution=1.74</b>) then you can re-run
AutoSol with the new values of your parameters like this:
</p><PRE style="face=courier">phenix.autosol autosol_hires.eff</PRE>
This command will do everything just the same as in your first run but
use all the data to 1.74 A.
</p>

<p><H5><I>Reading the log files for your AutoSol run file</I></H5></p>
<p>
While the AutoSol wizard is running, there are several places you can
look to see what is going on. The most important one is the overall
log file for the AutoSol run. This log file is located in:
</p><PRE style="face=courier">AutoSol_run_1_/AutoSol_run_1_1.log</PRE>
for run <b>1</b> of AutoSol.  (The second <b>1</b> in this log file
name will be incremented if you stop this run in the middle and
restart it with a command like <b>phenix.autosol run=1</b>).</p>
<p>
The <b>AutoSol_run_1_1.log</b> file is a running summary of what the
AutoSol Wizard is doing.  Here are a few of the key sections of the log
files produced for the <b>p9</b> SAD dataset.
</p>

<p><H5> Summary of the command-line arguments</H5></p>
<p>  Near the top of the log file 
you will find:
</p><PRE style="face=courier">------------------------------------------------------------
Starting AutoSol with the command:

phenix.autosol seq_file=seq.dat sites=4 atom_type=Se data=p9_se_w2.sca sg=I4   \
cell='113.949 113.949  32.474 90.000  90.000  90.00' resolution=2.4   \
thoroughness=quick
</PRE>
<p>This is just a repeat of how you ran AutoSol; you can copy it and paste it 
into the command line to repeat this run.
</p>

<p><H5> ImportRawData.  </H5></p>
<p>
The input data file <b>p9_se_w2.sca</b> is in unmerged
Scalepack format. The AutoSol wizard converts everything to premerged
Scalepack format before proceeding. Here is where the AutoSol Wizard
identifies the format and then calls the ImportRawData Wizard:
</p><PRE style="face=courier">HKLIN ENTRY:  p9_se_w2.sca
FILE TYPE scalepack_no_merge_original_index
GUESS FILE TYPE MERGE TYPE sca unmerged
LABELS['I', 'SIGI']
CONTENTS: ['p9_se_w2.sca', 'sca', 'unmerged', 'I 41', None, None, ['I', 'SIGI']]
Converting the files ['p9_se_w2.sca'] to sca format before proceeding
Running import directly...
WIZARD:  ImportRawData
PHENIX VERSION:  1.3b  of  27-06-2007
</PRE>

<p><H5>Using the datafiles converted to premerged format.</H5></p>
<p>
After completing the ImportRawData step, the AutoSol Wizard goes back to
the beginning, but uses the newly-converted file <b>p9_se_w2_PHX.sca</b>:
</p><PRE style="face=courier">HKLIN ENTRY:  AutoSol_run_1_/p9_se_w2_PHX.sca
FILE TYPE scalepack_merge
GUESS FILE TYPE MERGE TYPE sca premerged
LABELS['IPLUS', 'SIGIPLUS', 'IMINU', 'SIGIMINU']
Unit cell: (113.949, 113.949, 32.474, 90, 90, 90)
Space group: I 4 (No. 79)
CONTENTS: ['AutoSol_run_1_/p9_se_w2_PHX.sca', 'sca', 'premerged', 'I 4', 
[113.949, 113.949, 32.473999999999997, 90.0, 90.0, 90.0], 
1.7443432606877809, ['IPLUS', 'SIGIPLUS', 'IMINU', 'SIGIMINU']]
Total of 1 input data files
</PRE>

<p><H5>Guessing cell contents</H5></p>
<p>  The AutoSol Wizard uses the sequence information
in your sequence file (<b>seq.dat</b>) and the cell parameters and space
group to guess the number of NCS copies and the solvent fraction, and the
number of total methionines (approximately equal to the number of heavy-atom
sites for SeMet proteins):
</p><PRE style="face=courier"> AutoSol_guess_setup_for_scaling  AutoSol  Run 1 Tue Jul  3 20:38:55 2007

Solvent fraction and resolution and ha types/scatt fact
This is the last dataset to scale
Guessing setup for scaling dataset 1
SG I 4
cell [113.949, 113.949, 32.473999999999997, 90.0, 90.0, 90.0]
Number of residues in unique chains in seq file: 139
Unit cell: (113.949, 113.949, 32.474, 90, 90, 90)
Space group: I 4 (No. 79)
CELL VOLUME :421654.580793
N_EQUIV:8
GUESS OF NCS COPIES: 1
SOLVENT FRACTION ESTIMATE: 0.64
Total residues:139
Total Met:4
resolution estimate: 2.4
</PRE>

<p><H5>Running phenix.xtriage</H5></p>
<p>  The AutoSol Wizard automatically 
runs phenix.xtriage on each of your input datafiles to analyze them
for twinning, outliers, translational symmetry, and other special 
conditions that you should be aware of.  You can read more about
xtriage in <a href="xtriage.htm">Data quality assessment 
with phenix.xtriage</a>.  Part of the summary output from xtriage for this
dataset looks like this:
</p><PRE style="face=courier"> The largest off-origin peak in the Patterson function is 6.49% of the
height of the origin peak. No significant pseudotranslation is detected.

The results of the L-test indicate that the intensity statistics
behave as expected. No twinning is suspected.
</PRE>

<p><H5>Testing for anisotropy in the data</H5></p>
<p>  The AutoSol Wizard tests
for anisotropy by determining the range of effective anisotropic B values
along the principal lattice directions. If this range is large and the ratio
of the largest to the smallest value is also large then the data are by default
corrected to make the anisotropy small 
(see <a href="autosol.htm#anch26">
Analyzing and scaling the data</a>
in the AutoSol web page for more discussion
of the anisotropy correction). In the <b>p9</b> case, the range of
anisotropic B values is small and no correction is made:
</p><PRE style="face=courier">Range of aniso B:  15.67 26.14
Not using aniso-corrected data files as the range of aniso b  is only  
10.47  and 'correct_aniso' is not set
</PRE>

<p><H5>Choosing datafiles with high signal-to-noise</H5></p>
<p>  During scaling,
the AutoSol Wizard estimates the signal-to-noise in each datafile and
the resolution where there is significant signal-to-noise (above 0.3:1
signal-to-noise).  You can see this analysis in the log file 
<b>dataset_scale_1.log</b> for dataset 1.  In this case, 
the signal-to-noise is 1.4 to a resolution of 2.4 A:
</p><PRE style="face=courier">FILE DATA:AutoSol_run_1_/p9_se_w2_PHX.sca sn: 1.420786
</PRE>
<p><H5>Running HYSS to find the heavy-atom substructure</H5></p>
<p> The HYSS (hybrid substructure
search) procedure for heavy-atom searching uses a combination of a Patterson
search for 2-site solutions with direct methods recycling. The search ends
when the same solution is found beginning with several different starting
points.  The HYSS log files are named after the datafile that they
are based on and the type of differences (ano, iso) that are being used.
In this <b>p9</b> SAD dataset, the HYSS logfile is 
<b>p9_se_w2_PHX.sca_ano_1.sca_hyss.log</b>.  The key part of this HYSS log file
is:
</p><PRE style="face=courier"> Entering search loop:

p = peaklist index in Patterson map
f = peaklist index in two-site translation function
cc = correlation coefficient after extrapolation scan
r = number of dual-space recycling cycles
cc = final correlation coefficient

p=000 f=000 cc=0.392 r=015 cc=0.532 [ best cc: 0.532 ]
p=000 f=001 cc=0.381 r=015 cc=0.532 [ best cc: 0.532 0.532 ]
Number of matching sites of top 2 structures: 6

</PRE>
Here a correlation coefficient of 0.5 is very good (0.1 is hopeless,
0.2 is possible, 0.3 is good) and 6 sites were found that matched in
the first two tries. The program continues until 5 structures all have
matching sites, then ends and prints out the final correlations, after
taking the top 4 sites.

<p><H5>Scoring heavy-atom solutions</H5></p>
<p> The AutoSol Wizard scores heavy-atom
solutions based on four criteria. The first two are from
density modification. One of these is 
the correlation of the experimental phases with the map-probability phases
obtained in statistical density modification (<b>CC</b>). Good values for
this criteria are usually in the range of 0.4-0.6. The second is the R-factor
during the first cycle of density modification (<b>RFACTOR</b>). Good values
for this are anything less than 0.6.  The third criterion is the skew
of the electron density in the map (<b>SKEW</b>). 
Good values for the skew are anything
greater than 0.1. Typically in a SAD structure determination, the heavy-atom
solution with the correct hand has a positive skew and the one with the 
inverse hand has a negative skew. The last criterion is the figure of merit
of phasing (<b>FOM</b>), 
essentially a measure of the consistency of the data and
the heavy-atom model. For SAD datasets, Phaser is used for calculating
phases. For a SAD dataset, a figure of merit of 0.3 is
acceptable, 0.4 is fine and anything above 0.5 is very good.
 The scores are listed in the AutoSol log file:
</p><PRE style="face=courier">
Scoring for this solution now...

AutoSol_run_1_/TEMP0/resolve.scores CC 0.6260929
AutoSol_run_1_/TEMP0/resolve.scores RFACTOR 0.4817838
AutoSol_run_1_/TEMP0/resolve.scores SKEW 0.2500097
AutoSol_run_1_/TEMP0/resolve.scores FOM 0.557

CC-EST (BAYES-CC) CC : 68.4 +/- 27.5
CC-EST (BAYES-CC) RFACTOR : 58.2 +/- 36.6
CC-EST (BAYES-CC) SKEW : 54.5 +/- 17.8
CC-EST (BAYES-CC) FOM : 43.6 +/- 44.1
ESTIMATED MAP CC x 100:  59.6 +/- 8.4

</PRE>
The <b>ESTIMATED MAP CC x 100</b> is an estimate of the quality of
the experimental electron density map (not the density-modified one).
A set of real structures was used to calibrate the range of values of each
score that were obtained for phases with varying quality.  The resulting
probability distributions are used above to estimate the correlation between
the experimental map and an ideal map for this structure. Then all the
estimates are combined to yield an overall Bayesian estimate of the map
quality.  These are reported as CC x 100 +/- 2SD.  These estimated map
CC values are usually fairly close, so as the estimate is
 59.6 +/- 8.4, you can be quite confident that your structure is 
not only solved but that you 
will have a good map when it is density modified.


<p><H5>Final phasing with Phaser</H5></p>
Once the best heavy-atom solution or solutions are chosen based on 
<b>ESTIMATED MAP CC x 100</b>, these are used in a final round of phasing with
Phaser (for SAD phasing). The log file from phasing for solution <b>1</b> is in
<b>phaser_1.log</b>.  Here is the final part of the output from this log file,
showing the refined coordinates, occupancies, thermal (B) factors for
the 4 sites, along with the refined scattering factors (in this case
only f&quot; is refined), and the final figure of merit of phasing (0.548):
</p><PRE style="face=courier">
   Atom Parameters: 4 atoms in list
        X      Y      Z      O      B      (AnisoB) M Atomtype
   #1    0.180 -0.113 -0.681  1.144   23.2 ( ---- ) 1 SE
   #2    0.685 -0.238 -0.710  0.978   22.6 (+16.60) 1 SE
   #3    0.665 -0.206 -0.774  1.004   27.5 (+16.47) 1 SE
   #5    0.028  0.758  0.906  0.182   23.5 ( ---- ) 1 SE

   Scattering Parameters:
    Atom             f"           (f')
      SE         5.5375        -8.0000


   Figures of Merit
   ----------------
   Bin Resolution   Acentric     Centric      Single       Total
                    Number FOM   Number FOM   Number FOM   Number FOM
   ALL  28.49- 2.40   7502 0.592    874 0.140     51 0.056   8427 0.542

   log-likelihood gain -90102

</PRE>

<p><H5>Statistical density modification with RESOLVE</H5></p>
<p>After SAD phases are calculated with Phaser, the AutoSol Wizard
uses RESOLVE density modification to improve the quality of the
electron density map.  The statistical density modification 
in RESOLVE takes advantage of the flatness of the solvent region
and the expected distribution of electron density in the region
containing the macromolecule, as well as any NCS that can be found
from the heavy-atom substructure. The weighted structure factors and
phases (FWT, PHWT) from Phaser are used to calculate the starting
map for RESOLVE, and the experimental structure factor amplitudes (FP)
and SAD Hendrickson-Lattman coefficients from Phaser are used
in the density modification process. The output from RESOLVE for solution 1
can be found in <b>resolve_1.log</b>.  Here are key sections of this
output. </p>
<p>First, the plot of how many points in the &quot;protein&quot; region
of the map have each possible value of electron density. The
plot below is normalized so that a density of zero is the mean of 
the solvent region, and the standard deviation of the density in the
map is 1.0. A perfect map has a lot of points with density slightly less
than zero on this scale (the points between atoms) and a few points
with very high density (the points near atoms), and no points with very
negative density.  Such a map has a very high skew (think 
&quot;skewed off to the right&quot;). This map is good, with a positive
skew, though it is not perfect.
</p><PRE style="face=courier">

 Plot of Observed (o) and model (x) electron density distributions for protein
 region, where the model distribution is given by,
  p_model(beta*(rho+offset)) = p_ideal(rho)
 and then convoluted with a gaussian with width of sigma
 where sigma, offset and beta are given below under "Error estimate."


                          0.02..................................................
                              .                   .                            .
                              .                   .                            .
                              .             xxxxxxxxo                          .
                              .          oxxo o   oxxo                         .
                              .          x        .  xxo                       .
                              .        ox         .    xx                      .
                p(rho)        .       ox          .      xoo                   .
                              .      ox           .       xxoo                 .
                              .     xx            .         xxxo               .
                              .    x              .            xxo             .
                              .  oxx              .              xxxx          .
                              . xx                .                 xxxx       .
                              xx                  .                    oxxxx   .
                              x                   .                       ooxxxx
                         0.0  x................................................x

                             -2        -1         0         1         2        3

                                  normalized rho (0 = mean of solvent region)


</PRE>
<p>After density modification is complete, this plot becomes much more like
one from a perfect structure:
</p><PRE style="face=courier">
                          0.03..................................................
                              .                   .                            .
                              .            x      .                            .
                              .          xxoxxx   .                            .
                              .        xxo oooxxo .                            .
                              .       oo        xx.                            .
                              .      ox          xoo                           .
                p(rho)        .      x            xxo                          .
                              .    ox             . xx                         .
                              .    x              .  xxx                       .
                              .   xx              .     xxxxo                  .
                              .  xx               .        oxxxxxxx            .
                              . xo                .            ooooxxxxxx      .
                              xx                  .                  o  oxxxxxoo
                              o                   .                          oxo
                         0.0  o................................................x

                             -2        -1         0         1         2        3

                                  normalized rho (0 = mean of solvent region)

</PRE>
<p>The key statistic from this RESOLVE density modification is the R-factor
for comparison of observed structure factor amplitudes (FP) with those
calculated from the density modification procedure (FC).
In this <b>p9</b> SAD phasing the R-factor is very low: 
</p><PRE style="face=courier">
 Overall R-factor for FC vs FP: 0.239 for       8422 reflections
</PRE>
An acceptable value is anything below 0.35; below 0.30 is good.


<p><H5>Generation of FreeR flags</H5></p>
<p>The AutoSol Wizard will create a set of free R flags indicating which 
reflections are not to be used in refinement. By default 5% of reflections
(up to a maximum of 2000)
are reserved for this test set. 
If you want to supply a reflection file <b>hires.mtz</b> that has 
higher resolution than the data used to solve the structure, 
or has a test set already marked,
then you can do this with the keyword <b>input_refinement_file=hires.mtz</b>.

The files to be used for model-building
and refinement are listed in the AutoSol log file:
</p><PRE style="face=courier"> Generating FreeR flags...
Saving  exptl_fobs_phases_freeR_flags_1.mtz  for refinement
THE FILE AutoSol_run_1_/resolve_1.mtz will be used for model-building
THE FILE exptl_fobs_phases_freeR_flags_1.mtz will be used for refinement
</PRE>

<p><H5>Model-building with RESOLVE</H5></p>
<p>The AutoSol Wizard by default uses RESOLVE to build an atomic model
of your structure.  It will guess from your sequence file whether the 
structure is protein or RNA or DNA (but you can tell it if you want
with (<b>chain_type=PROTEIN</b>). The model-building carried out in
the AutoSol Wizard is essentially the same as one cycle of
model-building with the AutoBuild Wizard (see the web page
<a href="autobuild.htm">
Automated Model Building and Rebuilding with AutoBuild</a>, 
except that if you specify
<b>thoroughness=quick</b> as we have in this example, the model-building is
done less comprehensively to speed things up.
</p>
<p>In this case the model-building produces an initial model with 115 
residues built and side chains assigned to 77, and which refines to
an R-factor of 0.32 and a free R-factor of 0.35:
</p><PRE style="face=courier">Model 1: Residues built=115  placed=77  Chains=7  Model-map CC=0.69
This is new best model with cc =  0.69
Refining model:  Build_1.pdb
Refined model: AutoSol_run_1_/TEMP0/refine_1.pdb  R/Rfree=0.32/0.35
</PRE>
After one model completion cycle (including extending ends of chains,
fitting loops, and building outside the region already built, the best
model built has 126 residues built, all assigned to sequence:
</p><PRE style="face=courier"> 
Model 7: Residues built=126  placed=126  Chains=0  Model-map CC=0.82
This is new best model with cc =  0.82
Refining model:  model_with_loops_8.pdb
Refined model: AutoSol_run_1_/TEMP0/refine_7.pdb  R/Rfree=0.21/0.23
</PRE>
This refined initial model is written out to <b>refine_7.pdb</b> in
the output directory.

<p><H5>The AutoSol_summary.dat summary file</H5></p>
A quick summary of the results of your AutoSol run is in the
<b>AutoSol_summary.dat</b> file in your output directory.  This file lists the
key files that were produced in your run of AutoSol (all these are in the
output directory) and some of the key statistics for the run, including
the scores for the heavy-atom substructure and the model-building and 
refinement statistics.  These statistics are listed for all the solutions
obtained, with the highest-scoring solutions first.  Here is part of the
summary for this <b>p9</b> SAD dataset:
</p><PRE style="face=courier"> -----------CURRENT SOLUTIONS FOR RUN 1 : -------------------
 *** FILES ARE IN THE DIRECTORY: AutoSol_run_1_ ****



Solution # 2  BAYES-CC: 59.6 +/- 8.4 Dataset #1   FOM: 0.54 ----------------

Solution  2 using HYSS on AutoSol_run_1_/p9_se_w2_PHX.sca_ano_1.sca and taking inverse. Dataset #1
Dataset number: 1
Dataset type: sad
Datafiles used: ['AutoSol_run_1_/p9_se_w2_PHX.sca']
Sites: 4 (Already used for Phasing at resol of 2.4)      Refined Sites: 4
NCS information  in: AutoSol_2.ncs_spec
Experimental phases in: phaser_2.mtz
Experimental phases plus FreeR_flags for refinement in: exptl_fobs_phases_freeR_flags_2.mtz
Density-modified phases in: resolve_2.mtz
HA sites (PDB format) in: ha_2.pdb_formatted.pdb
Sequence file in: seq.dat
Model in: refine_7.pdb
  Residues built: 126
  Side-chains built: 126
  Chains: 0
  Overall model-map correlation: 0.82
  R/R-free: 0.21/0.23
Scaling logfile in: dataset_1_scale.log
HYSS logfile in: p9_se_w2_PHX.sca_ano_1.sca_hyss.log
Phasing logfile in: phaser_2.log
Density modification logfile in: resolve_2.log
Build logfile in: model_with_loops_8.log

 Score type:        CC       RFACTOR       SKEW       FOM
Raw scores:       0.63        0.48        0.25        0.56  
BAYES-CC:        68.42       58.15       54.50       43.55  

Refined heavy atom sites (fractional): 
xyz       0.180     -0.113     -0.681 
xyz       0.686     -0.238     -0.710 
xyz       0.665     -0.206     -0.774 
xyz       0.028      0.759      0.906 



</PRE>
<p><H5>How do I know if I have a good solution?</H5></p>
<p>Here are some of the things to look for to tell if you have obtained a 
correct solution:
<UL>
<LI>How much of the model was built? More than 60% is good.  If less than 
25% of the model is built, then it may be entirely incorrect. Have a
look at the model. If you see clear sets of parallel or antiparallel strands,
or if you see helices and strands with the expected relationships,
your model is going to be correct. If you see a lot of short fragments
everywhere, your model and solution is going to be incorrect.
How many side-chains were fitted to density? More than 25% is ok, 
more than 50% is very good. 
</LI>
<LI>What is the R-factor of the model? For a solution at moderate to high
resolution (2.5 A or better) the R-factor should be in the low 30's to be very 
good. For lower-resolution data, an R-factor in the low 40's is probably
largely correct but the model is not very good.
</LI>
<LI>What was the overall signal-to-noise in the data? Above 1 is good, below 0.5 is very low.
</LI>
<LI>What are the individual CC-BAYES estimates of map correlation
for your top solution? For a good
solution they are all around 50 or more, with 2SD uncertainties that are
about 10-20.
</LI>
<LI>What is the overall "ESTIMATED MAP CC x 100" of your top solution.  This
should also be 50 or more for a good solution. This is an estimate of the
map correlation before density modification, so if you have a lot of solvent
or several NCS-related copies in the asymmetric unit, then lower values may
still give you a good map.
</LI>
<LI>What is the difference in "ESTIMATED MAP CC x 100"
between the top solution and its
inverse? If this is large (more than the 2SD values for each)
 that is a good sign.
</LI>
</UL>


<p><H5>What to do next</H5></p>
<p>Once you have run AutoSol and have obtained a good solution and model,
the next thing to do is to run the AutoBuild Wizard.  If you run it
in the same directory where you ran AutoSol, the AutoBuild Wizard will pick
up where the AutoSol Wizard left off and carry out iterative model-building,
density modification and refinement to improve your model and map. 
See the web page <a href="autobuild.htm">
Automated Model Building and Rebuilding with AutoBuild</a> for details on
how to run AutoBuild.
</p><p>If you do not obtain a good solution, then it's not time to give
up yet. There are a number of standard things to try that may improve the
structure determination. Here are a few that you should always try:
<UL>
<LI>Have a careful look at all the output files. Work your way through
the main log file (e.g., <b>AutoSol_run_1_1.log</b>) and all the
other principal log files  in order beginning with 
scaling (<b>dataset_1_scale.log</b>), then looking at  heavy-atom
searching (<b>p9_se_w2_PHX.sca_ano_1.sca_hyss.log</b>),  phasing
(e.g., <b>phaser_1.log</b> or <b>phaser_xx.log</b> depending on
which solution <b>xx</b> was the top solution) and density modification (e.g., 
<b>resolve_xx.log</b>).  Is there anything 
strange or unusual in any of them that may give you a clue as to
what to try next?  For example did the phasing work well (high figure of
merit) yet the density modification failed? (Perhaps the hand is incorrect).
Was the solvent content estimated correctly? (You can specify it yourself
if you want).  What does the xtriage output say? Is there twinning or
strong translational symmetry? Are there problems with reflections near
ice rings? Are there many outlier reflections?
</LI>
<LI>Try a different resolution cutoff.  For example 0.5 A lower resolution than
you tried before.  Often the highest-resolution shells have little useful
information for structure solution (though the data may be useful in
refinement and density modification).
</LI>
<LI>Try a different rejection criterion for outliers. The default is
<b>ratio_out=3.0</b> (toss reflections with delta F more than 3 times the
rms delta F of all reflections in the shell).  Try instead <b>ratio_out=5.0</b>
to keep almost everything.
</LI>
<LI>If the heavy-atom substructure search did not yield plausible
solutions, try searching with HYSS using the command-line interface, and
vary the resolution and number of sites you look for. Can you find a solution
that has a higher <b>CC</b> than the one found in AutoSol? If so, you can
read your solution in to AutoSol with <b>sites_file=my_sites.pdb</b>.
</LI>
<LI>Was an anisotropy correction applied in AutoSol? If there is 
some anisotropy but no correction was applied, you can force AutoSol to 
apply the correction with <b>correct_aniso=True</b>.
</LI>
</UL>


</p><p><H5><U>Additional information</U></H5></p><p>
For details about the AutoSol Wizard, see <a href="autosol.htm"> 
Automated structure solution with AutoSol</a>.  For help on running
Wizards, see <a href="running-wizards.htm">
Running a Wizard from a GUI, the command-line, or a script</a>.
</p>


<!--REMARK PHENIX BODY END-->
